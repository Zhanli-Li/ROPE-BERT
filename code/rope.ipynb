{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fd03c-f5be-4220-ad0a-8a855023a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置代理环境变量\n",
    "os.environ['http_proxy'] = 'http://100.64.0.2:11080'\n",
    "os.environ['https_proxy'] = 'http://100.64.0.2:11080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119a0c6-7845-47b0-a7c9-b2a993d63f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae275715-d743-4b19-ae93-1cf53d195432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "parser = argparse.ArgumentParser(description='Bert Training')\n",
    "parser.add_argument('--num_heads', type=int, default=8, help='注意力头数')\n",
    "parser.add_argument('--num_layers', type=int, default=8, help='Transformer 层数')\n",
    "parser.add_argument('--d_model', type=int, default=768, help='模型维度')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='Dropout 概率')\n",
    "parser.add_argument('--d_ff', type=int, default=1024, help='前馈网络维度')\n",
    "parser.add_argument('--max_len', type=int, default=100, help='输入序列最大长度')\n",
    "parser.add_argument('--data_path', type=str, \n",
    "                    default=\"weibo_senti_100k.csv\", \n",
    "                    help='数据集路径')\n",
    "parser.add_argument('--batch_size', type=int, default=600, help='批大小')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='训练轮数')\n",
    "parser.add_argument('--lr', type=float, default=1e-5, help='学习率')\n",
    "parser.add_argument('--vocab_size', type=int, default=tokenizer.vocab_size, help='词汇表大小')\n",
    "parser.add_argument('--print_freq', type=int, default=1, help='打印损失频率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87210736-f1ee-4dc5-8d66-08eaf6a8b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiheadAttn(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout=0.1):\n",
    "       super(MutiheadAttn, self).__init__()\n",
    "       assert d_model % num_heads == 0\n",
    "       self.head_dim = d_model // num_heads  \n",
    "       self.q = nn.Linear(d_model, d_model)\n",
    "       self.k = nn.Linear(d_model, d_model)\n",
    "       self.v = nn.Linear(d_model, d_model)\n",
    "       \n",
    "       self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "    def forward(self,x):\n",
    "        q = self.q(x).view(x.size(0), x.size(1), -1, self.head_dim)\n",
    "        k = self.k(x).view(x.size(0), x.size(1), -1, self.head_dim)\n",
    "        v = self.v(x).view(x.size(0), x.size(1), -1, self.head_dim)\n",
    "        \n",
    "        q ,k , v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        logits = F.softmax(attn, dim=-1)\n",
    "        logits = self.dropout(logits)\n",
    "        attn = torch.matmul(logits, v)\n",
    "        output = attn.transpose(1,2).contiguous().view(x.size(0), x.size(1), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2ba1e-75b3-46ef-8874-9e85c6625363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f648c2-6d3b-48f5-a023-ff4a3ebb62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 创建一个位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))  # 计算每个维度的频率\n",
    "        \n",
    "        # 使用正弦和余弦计算位置编码\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度使用sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度使用cos\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # 在第0维加一个batch维度\n",
    "        self.register_buffer('pe', pe)  # 注册为buffer，确保不会被训练优化\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的形状是 (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 获取位置编码矩阵的前seq_len个位置编码\n",
    "        pe = self.pe[:, :seq_len, :]\n",
    "        \n",
    "        # 将位置编码添加到输入embedding上\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25faf8f-3840-47dc-a2ce-6cae11697263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=100, dropout=0.1):\n",
    "        super(Bert, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.ModuleList([MutiheadAttn(num_heads, d_model, dropout), FeedForward(d_model, d_ff, dropout)]) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)  # You can adjust this based on the task\n",
    "        self.final = nn.Linear(vocab_size, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        x = self.embeddings(x)  # Embedding layer: [batch_size, seq_len, d_model]\n",
    "        x = self.position_encoding(x)  # Add positional encoding\n",
    "\n",
    "        for attn, ff in self.encoder_layers:\n",
    "            # Multi-head attention\n",
    "            x = attn(x)\n",
    "            x = self.layer_norm(x + attn(x))  # Residual connection\n",
    "\n",
    "            # Feed-forward layer\n",
    "            x = ff(x)\n",
    "            x = self.layer_norm(x + ff(x))  # Residual connection\n",
    "        x = self.output_layer(x) # batch_size, seq_len, vocab_size\n",
    "        x = x.mean(dim=1)  # [batch_size, seq_len, vocab_size] -> [batch_size, vocab_size]\n",
    "        x = self.final(x)  # [batch_size, vocab_size] -> [batch_size, 2]\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b3845-b19c-4ab3-812e-a55ec5934725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee8e8b-9358-4375-9700-2809b8357944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moodtxtDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        import pandas as pd\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.texts = self.data['review'].tolist()\n",
    "        self.labels = self.data['label'].tolist()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        text = self.tokenizer.encode(text, add_special_tokens=True, max_length=100, padding='max_length', truncation=True)\n",
    "        text = torch.tensor(text, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0861c5-e50c-4a27-8a22-3e28993ee1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def train(args):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建完整数据集\n",
    "    full_dataset = moodtxtDataset(args.data_path)\n",
    "    print(set(full_dataset.data['label']))  # 查看标签分布\n",
    "    \n",
    "    # 按 8:2 划分训练集和测试集\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n",
    "                             num_workers=48, pin_memory=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                            num_workers=16, pin_memory=True)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = Bert(args.vocab_size, args.d_model, args.num_heads, \n",
    "                args.num_layers, args.d_ff, args.max_len, args.dropout).to(device)\n",
    "    \n",
    "    # 多卡并行\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=args.epoch, eta_min=1e-6)\n",
    "    \n",
    "    # 初始化结果存储\n",
    "    epoch_result_df = pd.DataFrame(columns=[\"Epoch\", \"Train Loss\", \"Test Loss\", \n",
    "                                           \"Train Acc\", \"Test Acc\", \"Train F1\", \"Test F1\",\n",
    "                                           \"Train Precision\", \"Test Precision\", \n",
    "                                           \"Train Recall\", \"Test Recall\", \"Learning Rate\"])\n",
    "    \n",
    "    batch_result_df = pd.DataFrame(columns=[\"Epoch\", \"Batch\", \"Loss\", \"Acc\",\n",
    "                                           \"Precision\", \"Recall\", \"F1\", \"Learning Rate\"])\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        # ======== 训练阶段 ========\n",
    "        model.train()\n",
    "        epoch_train_loss, total_correct = 0.0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 计算batch指标\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            batch_correct = (preds == labels).sum().item()\n",
    "            batch_acc = batch_correct / texts.size(0)\n",
    "            \n",
    "            # 转换数据到CPU计算指标\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            \n",
    "            # 记录batch结果\n",
    "            batch_result = {\n",
    "                \"Epoch\": epoch+1,\n",
    "                \"Batch\": batch_idx+1,\n",
    "                \"Loss\": loss.item(),\n",
    "                \"Acc\": batch_acc,\n",
    "                \"Precision\": precision_score(labels_np, preds_np, average='macro', zero_division=0),\n",
    "                \"Recall\": recall_score(labels_np, preds_np, average='macro', zero_division=0),\n",
    "                \"F1\": f1_score(labels_np, preds_np, average='macro', zero_division=0),\n",
    "                \"Learning Rate\": optimizer.param_groups[0]['lr']\n",
    "            }\n",
    "            batch_result_df = pd.concat([batch_result_df, pd.DataFrame([batch_result])], ignore_index=True)\n",
    "            \n",
    "            # 累积epoch指标\n",
    "            total_correct += batch_correct\n",
    "            epoch_train_loss += loss.item() * texts.size(0)\n",
    "            all_preds.extend(preds_np)\n",
    "            all_labels.extend(labels_np)\n",
    "            \n",
    "            # 打印进度\n",
    "            if (batch_idx + 1) % args.print_freq == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{args.epoch}] | Batch [{batch_idx+1}/{len(train_loader)}] | \"\n",
    "                      f\"Loss: {loss.item():.4f} | Acc: {batch_acc:.2%}\")\n",
    "        \n",
    "        # 计算训练集整体指标\n",
    "        train_acc = total_correct / len(train_dataset)\n",
    "        train_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        train_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        avg_train_loss = epoch_train_loss / len(train_dataset)\n",
    "        \n",
    "        # ======== 测试阶段 ========\n",
    "        model.eval()\n",
    "        test_loss, test_correct = 0.0, 0\n",
    "        test_preds, test_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, labels in test_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * labels.size(0)\n",
    "                \n",
    "                # 记录预测结果\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 计算测试集指标\n",
    "        test_acc = test_correct / len(test_dataset)\n",
    "        test_precision = precision_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "        test_recall = recall_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "        test_f1 = f1_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "        avg_test_loss = test_loss / len(test_dataset)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ======== 记录结果 ========\n",
    "        epoch_result_df = pd.concat([epoch_result_df, pd.DataFrame({\n",
    "            \"Epoch\": [epoch+1],\n",
    "            \"Train Loss\": [avg_train_loss],\n",
    "            \"Test Loss\": [avg_test_loss],\n",
    "            \"Train Acc\": [train_acc],\n",
    "            \"Test Acc\": [test_acc],\n",
    "            \"Train F1\": [train_f1],\n",
    "            \"Test F1\": [test_f1],\n",
    "            \"Train Precision\": [train_precision],\n",
    "            \"Test Precision\": [test_precision],\n",
    "            \"Train Recall\": [train_recall],\n",
    "            \"Test Recall\": [test_recall],\n",
    "            \"Learning Rate\": [optimizer.param_groups[0]['lr']]\n",
    "        })], ignore_index=True)\n",
    "        \n",
    "        # 打印epoch总结\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Acc: {train_acc:.2%} | \"\n",
    "              f\"Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | F1: {train_f1:.4f}\")\n",
    "        print(f\"Test  Loss: {avg_test_loss:.4f} | Acc: {test_acc:.2%} | \"\n",
    "              f\"Precision: {test_precision:.4f} | Recall: {test_recall:.4f} | F1: {test_f1:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\\n\")\n",
    "        \n",
    "        # 保存模型\n",
    "        torch.save(model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "                  f\"model_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # 保存结果\n",
    "    epoch_result_df.to_csv('epoch_training_metrics.csv', index=False)\n",
    "    batch_result_df.to_csv('batch_training_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb5809-467a-4544-bfd0-82d15744f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown_args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e943c7-7d73-4adc-bcda-180c689bff79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9adb5-cb4e-4032-8da5-136101a7506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # 初始化模型\n",
    "    model = Bert(args.vocab_size, args.d_model, args.num_heads, \n",
    "                args.num_layers, args.d_ff, args.max_len, args.dropout)\n",
    "    \n",
    "    # 加载预训练权重（关键新增部分）\n",
    "    model.load_state_dict(torch.load('model_epoch_2.pth'))  # 替换为你的权重路径\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 文本编码\n",
    "        text = tokenizer.encode(text, \n",
    "                              add_special_tokens=True,\n",
    "                              max_length=100,\n",
    "                              padding='max_length',\n",
    "                              truncation=True)\n",
    "        text = torch.tensor(text, dtype=torch.long).unsqueeze(0)\n",
    "        \n",
    "        # 前向传播\n",
    "        output = model(text)\n",
    "        print(output)\n",
    "        # 获取预测结果（修正了参数缺失问题）\n",
    "        preds = torch.argmax(output, dim=-1)  # 添加了output参数\n",
    "        \n",
    "        return preds\n",
    "predict('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122eaa5a-f15a-47ee-86c6-74833ac83005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c24f1-189c-4bc5-af13-4bde768c9e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a01911-59b6-44a0-81dd-8ad016409840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
