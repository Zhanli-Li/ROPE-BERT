{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2fd03c-f5be-4220-ad0a-8a855023a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置代理环境变量\n",
    "os.environ['http_proxy'] = 'http://100.64.0.2:11080'\n",
    "os.environ['https_proxy'] = 'http://100.64.0.2:11080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6119a0c6-7845-47b0-a7c9-b2a993d63f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae275715-d743-4b19-ae93-1cf53d195432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--print_freq'], dest='print_freq', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='打印损失频率', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "parser = argparse.ArgumentParser(description='Bert Training')\n",
    "parser.add_argument('--num_heads', type=int, default=8, help='注意力头数')\n",
    "parser.add_argument('--num_layers', type=int, default=8, help='Transformer 层数')\n",
    "parser.add_argument('--d_model', type=int, default=768, help='模型维度')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='Dropout 概率')\n",
    "parser.add_argument('--d_ff', type=int, default=1024, help='前馈网络维度')\n",
    "parser.add_argument('--max_len', type=int, default=100, help='输入序列最大长度')\n",
    "parser.add_argument('--data_path', type=str, \n",
    "                    default=\"weibo_senti_100k.csv\", \n",
    "                    help='数据集路径')\n",
    "parser.add_argument('--batch_size', type=int, default=500, help='批大小')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='训练轮数')\n",
    "parser.add_argument('--lr', type=float, default=1e-5, help='学习率')\n",
    "parser.add_argument('--vocab_size', type=int, default=tokenizer.vocab_size, help='词汇表大小')\n",
    "parser.add_argument('--print_freq', type=int, default=1, help='打印损失频率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87210736-f1ee-4dc5-8d66-08eaf6a8b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiheadAttn(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout=0.1):\n",
    "       super(MutiheadAttn, self).__init__()\n",
    "       assert d_model % num_heads == 0\n",
    "       self.head_dim = d_model // num_heads  \n",
    "       self.q = nn.Linear(d_model, d_model)\n",
    "       self.k = nn.Linear(d_model, d_model)\n",
    "       self.v = nn.Linear(d_model, d_model)\n",
    "       \n",
    "       self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "    def forward(self,x):\n",
    "        q = self.q(x).view(x.size(0), x.size(1), -1, self.head_dim)\n",
    "        k = self.k(x).view(x.size(0), x.size(1), -1, self.head_dim)\n",
    "        v = self.v(x).view(x.size(0), x.size(1), -1, self.head_dim)\n",
    "        \n",
    "        q ,k , v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        logits = F.softmax(attn, dim=-1)\n",
    "        logits = self.dropout(logits)\n",
    "        attn = torch.matmul(logits, v)\n",
    "        output = attn.transpose(1,2).contiguous().view(x.size(0), x.size(1), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd2ba1e-75b3-46ef-8874-9e85c6625363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f648c2-6d3b-48f5-a023-ff4a3ebb62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 创建一个位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))  # 计算每个维度的频率\n",
    "        \n",
    "        # 使用正弦和余弦计算位置编码\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度使用sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度使用cos\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # 在第0维加一个batch维度\n",
    "        self.register_buffer('pe', pe)  # 注册为buffer，确保不会被训练优化\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的形状是 (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 获取位置编码矩阵的前seq_len个位置编码\n",
    "        pe = self.pe[:, :seq_len, :]\n",
    "        \n",
    "        # 将位置编码添加到输入embedding上\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25faf8f-3840-47dc-a2ce-6cae11697263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=100, dropout=0.1):\n",
    "        super(Bert, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.ModuleList([MutiheadAttn(num_heads, d_model, dropout), FeedForward(d_model, d_ff, dropout)]) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)  # You can adjust this based on the task\n",
    "        self.final = nn.Linear(vocab_size, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        x = self.embeddings(x)  # Embedding layer: [batch_size, seq_len, d_model]\n",
    "        x = self.position_encoding(x)  # Add positional encoding\n",
    "\n",
    "        for attn, ff in self.encoder_layers:\n",
    "            # Multi-head attention\n",
    "            x = attn(x)\n",
    "            x = self.layer_norm(x + attn(x))  # Residual connection\n",
    "\n",
    "            # Feed-forward layer\n",
    "            x = ff(x)\n",
    "            x = self.layer_norm(x + ff(x))  # Residual connection\n",
    "        x = self.output_layer(x) # batch_size, seq_len, vocab_size\n",
    "        x = x.mean(dim=1)  # [batch_size, seq_len, vocab_size] -> [batch_size, vocab_size]\n",
    "        x = self.final(x)  # [batch_size, vocab_size] -> [batch_size, 2]\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69b3845-b19c-4ab3-812e-a55ec5934725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bee8e8b-9358-4375-9700-2809b8357944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moodtxtDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        import pandas as pd\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.texts = self.data['review'].tolist()\n",
    "        self.labels = self.data['label'].tolist()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        text = self.tokenizer.encode(text, add_special_tokens=True, max_length=100, padding='max_length', truncation=True)\n",
    "        text = torch.tensor(text, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0861c5-e50c-4a27-8a22-3e28993ee1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def train(args):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建完整数据集\n",
    "    full_dataset = moodtxtDataset(args.data_path)\n",
    "    print(set(full_dataset.data['label']))  # 查看标签分布\n",
    "    \n",
    "    # 按 8:2 划分训练集和测试集\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n",
    "                             num_workers=48, pin_memory=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                            num_workers=16, pin_memory=True)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = Bert(args.vocab_size, args.d_model, args.num_heads, \n",
    "                args.num_layers, args.d_ff, args.max_len, args.dropout).to(device)\n",
    "    \n",
    "    # 多卡并行\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=args.epoch, eta_min=1e-6)\n",
    "    \n",
    "    # 初始化结果存储\n",
    "    epoch_result_df = pd.DataFrame(columns=[\"Epoch\", \"Train Loss\", \"Test Loss\", \n",
    "                                           \"Train Acc\", \"Test Acc\", \"Train F1\", \"Test F1\",\n",
    "                                           \"Train Precision\", \"Test Precision\", \n",
    "                                           \"Train Recall\", \"Test Recall\", \"Learning Rate\"])\n",
    "    \n",
    "    batch_result_df = pd.DataFrame(columns=[\"Epoch\", \"Batch\", \"Loss\", \"Acc\",\n",
    "                                           \"Precision\", \"Recall\", \"F1\", \"Learning Rate\"])\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        # ======== 训练阶段 ========\n",
    "        model.train()\n",
    "        epoch_train_loss, total_correct = 0.0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 计算batch指标\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            batch_correct = (preds == labels).sum().item()\n",
    "            batch_acc = batch_correct / texts.size(0)\n",
    "            \n",
    "            # 转换数据到CPU计算指标\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            \n",
    "            # 记录batch结果\n",
    "            batch_result = {\n",
    "                \"Epoch\": epoch+1,\n",
    "                \"Batch\": batch_idx+1,\n",
    "                \"Loss\": loss.item(),\n",
    "                \"Acc\": batch_acc,\n",
    "                \"Precision\": precision_score(labels_np, preds_np, average='macro', zero_division=0),\n",
    "                \"Recall\": recall_score(labels_np, preds_np, average='macro', zero_division=0),\n",
    "                \"F1\": f1_score(labels_np, preds_np, average='macro', zero_division=0),\n",
    "                \"Learning Rate\": optimizer.param_groups[0]['lr']\n",
    "            }\n",
    "            batch_result_df = pd.concat([batch_result_df, pd.DataFrame([batch_result])], ignore_index=True)\n",
    "            \n",
    "            # 累积epoch指标\n",
    "            total_correct += batch_correct\n",
    "            epoch_train_loss += loss.item() * texts.size(0)\n",
    "            all_preds.extend(preds_np)\n",
    "            all_labels.extend(labels_np)\n",
    "            \n",
    "            # 打印进度\n",
    "            if (batch_idx + 1) % args.print_freq == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{args.epoch}] | Batch [{batch_idx+1}/{len(train_loader)}] | \"\n",
    "                      f\"Loss: {loss.item():.4f} | Acc: {batch_acc:.2%}\")\n",
    "        \n",
    "        # 计算训练集整体指标\n",
    "        train_acc = total_correct / len(train_dataset)\n",
    "        train_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        train_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        avg_train_loss = epoch_train_loss / len(train_dataset)\n",
    "        \n",
    "        # ======== 测试阶段 ========\n",
    "        model.eval()\n",
    "        test_loss, test_correct = 0.0, 0\n",
    "        test_preds, test_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, labels in test_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * labels.size(0)\n",
    "                \n",
    "                # 记录预测结果\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 计算测试集指标\n",
    "        test_acc = test_correct / len(test_dataset)\n",
    "        test_precision = precision_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "        test_recall = recall_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "        test_f1 = f1_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "        avg_test_loss = test_loss / len(test_dataset)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ======== 记录结果 ========\n",
    "        epoch_result_df = pd.concat([epoch_result_df, pd.DataFrame({\n",
    "            \"Epoch\": [epoch+1],\n",
    "            \"Train Loss\": [avg_train_loss],\n",
    "            \"Test Loss\": [avg_test_loss],\n",
    "            \"Train Acc\": [train_acc],\n",
    "            \"Test Acc\": [test_acc],\n",
    "            \"Train F1\": [train_f1],\n",
    "            \"Test F1\": [test_f1],\n",
    "            \"Train Precision\": [train_precision],\n",
    "            \"Test Precision\": [test_precision],\n",
    "            \"Train Recall\": [train_recall],\n",
    "            \"Test Recall\": [test_recall],\n",
    "            \"Learning Rate\": [optimizer.param_groups[0]['lr']]\n",
    "        })], ignore_index=True)\n",
    "        \n",
    "        # 打印epoch总结\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Acc: {train_acc:.2%} | \"\n",
    "              f\"Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | F1: {train_f1:.4f}\")\n",
    "        print(f\"Test  Loss: {avg_test_loss:.4f} | Acc: {test_acc:.2%} | \"\n",
    "              f\"Precision: {test_precision:.4f} | Recall: {test_recall:.4f} | F1: {test_f1:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\\n\")\n",
    "        \n",
    "        # 保存模型\n",
    "        torch.save(model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "                  f\"model_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # 保存结果\n",
    "    epoch_result_df.to_csv('sinepoch_training_metrics.csv', index=False)\n",
    "    batch_result_df.to_csv('sinbatch_training_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6cb5809-467a-4544-bfd0-82d15744f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown_args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17e943c7-7d73-4adc-bcda-180c689bff79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/1024/ipykernel_22743/1531228587.py:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  batch_result_df = pd.concat([batch_result_df, pd.DataFrame([batch_result])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Batch [1/191] | Loss: 0.6962 | Acc: 45.00%\n",
      "Epoch [1/10] | Batch [2/191] | Loss: 0.7789 | Acc: 47.40%\n",
      "Epoch [1/10] | Batch [3/191] | Loss: 0.7251 | Acc: 51.00%\n",
      "Epoch [1/10] | Batch [4/191] | Loss: 0.6951 | Acc: 50.20%\n",
      "Epoch [1/10] | Batch [5/191] | Loss: 0.7050 | Acc: 49.60%\n",
      "Epoch [1/10] | Batch [6/191] | Loss: 0.7137 | Acc: 48.80%\n",
      "Epoch [1/10] | Batch [7/191] | Loss: 0.7028 | Acc: 49.20%\n",
      "Epoch [1/10] | Batch [8/191] | Loss: 0.6948 | Acc: 46.00%\n",
      "Epoch [1/10] | Batch [9/191] | Loss: 0.7034 | Acc: 47.60%\n",
      "Epoch [1/10] | Batch [10/191] | Loss: 0.6963 | Acc: 52.40%\n",
      "Epoch [1/10] | Batch [11/191] | Loss: 0.7171 | Acc: 45.40%\n",
      "Epoch [1/10] | Batch [12/191] | Loss: 0.7009 | Acc: 47.40%\n",
      "Epoch [1/10] | Batch [13/191] | Loss: 0.6931 | Acc: 50.80%\n",
      "Epoch [1/10] | Batch [14/191] | Loss: 0.6968 | Acc: 50.00%\n",
      "Epoch [1/10] | Batch [15/191] | Loss: 0.7018 | Acc: 50.00%\n",
      "Epoch [1/10] | Batch [16/191] | Loss: 0.6941 | Acc: 53.00%\n",
      "Epoch [1/10] | Batch [17/191] | Loss: 0.6959 | Acc: 51.60%\n",
      "Epoch [1/10] | Batch [18/191] | Loss: 0.6945 | Acc: 51.00%\n",
      "Epoch [1/10] | Batch [19/191] | Loss: 0.6933 | Acc: 50.00%\n",
      "Epoch [1/10] | Batch [20/191] | Loss: 0.6930 | Acc: 49.80%\n",
      "Epoch [1/10] | Batch [21/191] | Loss: 0.6945 | Acc: 50.00%\n",
      "Epoch [1/10] | Batch [22/191] | Loss: 0.6965 | Acc: 49.60%\n",
      "Epoch [1/10] | Batch [23/191] | Loss: 0.6982 | Acc: 48.20%\n",
      "Epoch [1/10] | Batch [24/191] | Loss: 0.6934 | Acc: 49.20%\n",
      "Epoch [1/10] | Batch [25/191] | Loss: 0.6919 | Acc: 55.40%\n",
      "Epoch [1/10] | Batch [26/191] | Loss: 0.6922 | Acc: 51.00%\n",
      "Epoch [1/10] | Batch [27/191] | Loss: 0.6985 | Acc: 48.40%\n",
      "Epoch [1/10] | Batch [28/191] | Loss: 0.6936 | Acc: 50.80%\n",
      "Epoch [1/10] | Batch [29/191] | Loss: 0.6957 | Acc: 47.00%\n",
      "Epoch [1/10] | Batch [30/191] | Loss: 0.6905 | Acc: 56.60%\n",
      "Epoch [1/10] | Batch [31/191] | Loss: 0.6901 | Acc: 52.40%\n",
      "Epoch [1/10] | Batch [32/191] | Loss: 0.6855 | Acc: 55.20%\n",
      "Epoch [1/10] | Batch [33/191] | Loss: 0.7057 | Acc: 48.60%\n",
      "Epoch [1/10] | Batch [34/191] | Loss: 0.6940 | Acc: 52.20%\n",
      "Epoch [1/10] | Batch [35/191] | Loss: 0.6985 | Acc: 47.40%\n",
      "Epoch [1/10] | Batch [36/191] | Loss: 0.6920 | Acc: 51.20%\n",
      "Epoch [1/10] | Batch [37/191] | Loss: 0.6924 | Acc: 51.60%\n",
      "Epoch [1/10] | Batch [38/191] | Loss: 0.6938 | Acc: 51.20%\n",
      "Epoch [1/10] | Batch [39/191] | Loss: 0.6974 | Acc: 50.40%\n",
      "Epoch [1/10] | Batch [40/191] | Loss: 0.6867 | Acc: 53.40%\n",
      "Epoch [1/10] | Batch [41/191] | Loss: 0.6833 | Acc: 55.20%\n",
      "Epoch [1/10] | Batch [42/191] | Loss: 0.6830 | Acc: 58.20%\n",
      "Epoch [1/10] | Batch [43/191] | Loss: 0.6813 | Acc: 64.00%\n",
      "Epoch [1/10] | Batch [44/191] | Loss: 0.6836 | Acc: 55.40%\n",
      "Epoch [1/10] | Batch [45/191] | Loss: 0.6920 | Acc: 51.80%\n",
      "Epoch [1/10] | Batch [46/191] | Loss: 0.6680 | Acc: 67.20%\n",
      "Epoch [1/10] | Batch [47/191] | Loss: 0.6804 | Acc: 56.40%\n",
      "Epoch [1/10] | Batch [48/191] | Loss: 0.6716 | Acc: 59.20%\n",
      "Epoch [1/10] | Batch [49/191] | Loss: 0.6501 | Acc: 66.40%\n",
      "Epoch [1/10] | Batch [50/191] | Loss: 0.6454 | Acc: 64.20%\n",
      "Epoch [1/10] | Batch [51/191] | Loss: 0.6509 | Acc: 60.00%\n",
      "Epoch [1/10] | Batch [52/191] | Loss: 0.6046 | Acc: 78.40%\n",
      "Epoch [1/10] | Batch [53/191] | Loss: 0.5937 | Acc: 74.40%\n",
      "Epoch [1/10] | Batch [54/191] | Loss: 0.5637 | Acc: 78.00%\n",
      "Epoch [1/10] | Batch [55/191] | Loss: 0.5537 | Acc: 76.00%\n",
      "Epoch [1/10] | Batch [56/191] | Loss: 0.6152 | Acc: 66.00%\n",
      "Epoch [1/10] | Batch [57/191] | Loss: 0.5487 | Acc: 75.80%\n",
      "Epoch [1/10] | Batch [58/191] | Loss: 0.6091 | Acc: 70.00%\n",
      "Epoch [1/10] | Batch [59/191] | Loss: 0.5963 | Acc: 71.60%\n",
      "Epoch [1/10] | Batch [60/191] | Loss: 0.6070 | Acc: 69.80%\n",
      "Epoch [1/10] | Batch [61/191] | Loss: 0.5398 | Acc: 76.80%\n",
      "Epoch [1/10] | Batch [62/191] | Loss: 0.5156 | Acc: 78.20%\n",
      "Epoch [1/10] | Batch [63/191] | Loss: 0.5264 | Acc: 77.60%\n",
      "Epoch [1/10] | Batch [64/191] | Loss: 0.5678 | Acc: 74.00%\n",
      "Epoch [1/10] | Batch [65/191] | Loss: 0.5375 | Acc: 76.80%\n",
      "Epoch [1/10] | Batch [66/191] | Loss: 0.5466 | Acc: 75.20%\n",
      "Epoch [1/10] | Batch [67/191] | Loss: 0.5446 | Acc: 75.80%\n",
      "Epoch [1/10] | Batch [68/191] | Loss: 0.5406 | Acc: 76.80%\n",
      "Epoch [1/10] | Batch [69/191] | Loss: 0.5230 | Acc: 78.00%\n",
      "Epoch [1/10] | Batch [70/191] | Loss: 0.5038 | Acc: 80.00%\n",
      "Epoch [1/10] | Batch [71/191] | Loss: 0.5721 | Acc: 72.00%\n",
      "Epoch [1/10] | Batch [72/191] | Loss: 0.4891 | Acc: 81.60%\n",
      "Epoch [1/10] | Batch [73/191] | Loss: 0.5582 | Acc: 74.80%\n",
      "Epoch [1/10] | Batch [74/191] | Loss: 0.4966 | Acc: 81.20%\n",
      "Epoch [1/10] | Batch [75/191] | Loss: 0.4939 | Acc: 80.80%\n",
      "Epoch [1/10] | Batch [76/191] | Loss: 0.5216 | Acc: 78.60%\n",
      "Epoch [1/10] | Batch [77/191] | Loss: 0.4787 | Acc: 82.80%\n",
      "Epoch [1/10] | Batch [78/191] | Loss: 0.4974 | Acc: 80.20%\n",
      "Epoch [1/10] | Batch [79/191] | Loss: 0.4598 | Acc: 85.80%\n",
      "Epoch [1/10] | Batch [80/191] | Loss: 0.4882 | Acc: 81.80%\n",
      "Epoch [1/10] | Batch [81/191] | Loss: 0.5148 | Acc: 78.80%\n",
      "Epoch [1/10] | Batch [82/191] | Loss: 0.4810 | Acc: 82.40%\n",
      "Epoch [1/10] | Batch [83/191] | Loss: 0.5263 | Acc: 76.20%\n",
      "Epoch [1/10] | Batch [84/191] | Loss: 0.4830 | Acc: 82.20%\n",
      "Epoch [1/10] | Batch [85/191] | Loss: 0.4794 | Acc: 82.60%\n",
      "Epoch [1/10] | Batch [86/191] | Loss: 0.4662 | Acc: 84.00%\n",
      "Epoch [1/10] | Batch [87/191] | Loss: 0.4523 | Acc: 85.20%\n",
      "Epoch [1/10] | Batch [88/191] | Loss: 0.4987 | Acc: 80.80%\n",
      "Epoch [1/10] | Batch [89/191] | Loss: 0.4415 | Acc: 87.20%\n",
      "Epoch [1/10] | Batch [90/191] | Loss: 0.4665 | Acc: 83.00%\n",
      "Epoch [1/10] | Batch [91/191] | Loss: 0.4843 | Acc: 82.00%\n",
      "Epoch [1/10] | Batch [92/191] | Loss: 0.4366 | Acc: 87.80%\n",
      "Epoch [1/10] | Batch [93/191] | Loss: 0.4581 | Acc: 85.40%\n",
      "Epoch [1/10] | Batch [94/191] | Loss: 0.4537 | Acc: 85.00%\n",
      "Epoch [1/10] | Batch [95/191] | Loss: 0.4536 | Acc: 86.00%\n",
      "Epoch [1/10] | Batch [96/191] | Loss: 0.4601 | Acc: 84.40%\n",
      "Epoch [1/10] | Batch [97/191] | Loss: 0.4748 | Acc: 83.40%\n",
      "Epoch [1/10] | Batch [98/191] | Loss: 0.4503 | Acc: 85.60%\n",
      "Epoch [1/10] | Batch [99/191] | Loss: 0.4541 | Acc: 85.00%\n",
      "Epoch [1/10] | Batch [100/191] | Loss: 0.4565 | Acc: 84.80%\n",
      "Epoch [1/10] | Batch [101/191] | Loss: 0.4464 | Acc: 86.00%\n",
      "Epoch [1/10] | Batch [102/191] | Loss: 0.4212 | Acc: 89.60%\n",
      "Epoch [1/10] | Batch [103/191] | Loss: 0.4448 | Acc: 86.60%\n",
      "Epoch [1/10] | Batch [104/191] | Loss: 0.4187 | Acc: 89.00%\n",
      "Epoch [1/10] | Batch [105/191] | Loss: 0.4487 | Acc: 85.60%\n",
      "Epoch [1/10] | Batch [106/191] | Loss: 0.4165 | Acc: 89.20%\n",
      "Epoch [1/10] | Batch [107/191] | Loss: 0.4307 | Acc: 88.20%\n",
      "Epoch [1/10] | Batch [108/191] | Loss: 0.4524 | Acc: 85.80%\n",
      "Epoch [1/10] | Batch [109/191] | Loss: 0.4295 | Acc: 88.00%\n",
      "Epoch [1/10] | Batch [110/191] | Loss: 0.4221 | Acc: 89.20%\n",
      "Epoch [1/10] | Batch [111/191] | Loss: 0.4207 | Acc: 88.80%\n",
      "Epoch [1/10] | Batch [112/191] | Loss: 0.4312 | Acc: 88.20%\n",
      "Epoch [1/10] | Batch [113/191] | Loss: 0.4391 | Acc: 87.00%\n",
      "Epoch [1/10] | Batch [114/191] | Loss: 0.4378 | Acc: 87.20%\n",
      "Epoch [1/10] | Batch [115/191] | Loss: 0.4241 | Acc: 88.20%\n",
      "Epoch [1/10] | Batch [116/191] | Loss: 0.4345 | Acc: 87.20%\n",
      "Epoch [1/10] | Batch [117/191] | Loss: 0.4262 | Acc: 88.80%\n",
      "Epoch [1/10] | Batch [118/191] | Loss: 0.4207 | Acc: 88.60%\n",
      "Epoch [1/10] | Batch [119/191] | Loss: 0.4259 | Acc: 88.80%\n",
      "Epoch [1/10] | Batch [120/191] | Loss: 0.3992 | Acc: 91.40%\n",
      "Epoch [1/10] | Batch [121/191] | Loss: 0.4105 | Acc: 90.40%\n",
      "Epoch [1/10] | Batch [122/191] | Loss: 0.4388 | Acc: 87.00%\n",
      "Epoch [1/10] | Batch [123/191] | Loss: 0.4345 | Acc: 87.60%\n",
      "Epoch [1/10] | Batch [124/191] | Loss: 0.4254 | Acc: 88.60%\n",
      "Epoch [1/10] | Batch [125/191] | Loss: 0.4388 | Acc: 86.60%\n",
      "Epoch [1/10] | Batch [126/191] | Loss: 0.4112 | Acc: 90.20%\n",
      "Epoch [1/10] | Batch [127/191] | Loss: 0.4198 | Acc: 89.00%\n",
      "Epoch [1/10] | Batch [128/191] | Loss: 0.4220 | Acc: 89.00%\n",
      "Epoch [1/10] | Batch [129/191] | Loss: 0.4144 | Acc: 89.60%\n",
      "Epoch [1/10] | Batch [130/191] | Loss: 0.4582 | Acc: 83.80%\n",
      "Epoch [1/10] | Batch [131/191] | Loss: 0.4245 | Acc: 88.20%\n",
      "Epoch [1/10] | Batch [132/191] | Loss: 0.4177 | Acc: 89.40%\n",
      "Epoch [1/10] | Batch [133/191] | Loss: 0.4262 | Acc: 88.40%\n",
      "Epoch [1/10] | Batch [134/191] | Loss: 0.4467 | Acc: 86.00%\n",
      "Epoch [1/10] | Batch [135/191] | Loss: 0.4334 | Acc: 88.20%\n",
      "Epoch [1/10] | Batch [136/191] | Loss: 0.4299 | Acc: 87.60%\n",
      "Epoch [1/10] | Batch [137/191] | Loss: 0.4261 | Acc: 88.00%\n",
      "Epoch [1/10] | Batch [138/191] | Loss: 0.4243 | Acc: 88.20%\n",
      "Epoch [1/10] | Batch [139/191] | Loss: 0.4219 | Acc: 88.80%\n",
      "Epoch [1/10] | Batch [140/191] | Loss: 0.4620 | Acc: 83.60%\n",
      "Epoch [1/10] | Batch [141/191] | Loss: 0.4171 | Acc: 89.40%\n",
      "Epoch [1/10] | Batch [142/191] | Loss: 0.4303 | Acc: 87.40%\n",
      "Epoch [1/10] | Batch [143/191] | Loss: 0.4534 | Acc: 85.00%\n",
      "Epoch [1/10] | Batch [144/191] | Loss: 0.3997 | Acc: 90.60%\n",
      "Epoch [1/10] | Batch [145/191] | Loss: 0.4489 | Acc: 85.40%\n",
      "Epoch [1/10] | Batch [146/191] | Loss: 0.4520 | Acc: 85.40%\n",
      "Epoch [1/10] | Batch [147/191] | Loss: 0.4189 | Acc: 88.60%\n",
      "Epoch [1/10] | Batch [148/191] | Loss: 0.4165 | Acc: 89.00%\n",
      "Epoch [1/10] | Batch [149/191] | Loss: 0.4284 | Acc: 88.00%\n",
      "Epoch [1/10] | Batch [150/191] | Loss: 0.4234 | Acc: 88.20%\n",
      "Epoch [1/10] | Batch [151/191] | Loss: 0.4395 | Acc: 87.20%\n",
      "Epoch [1/10] | Batch [152/191] | Loss: 0.4019 | Acc: 90.20%\n",
      "Epoch [1/10] | Batch [153/191] | Loss: 0.4094 | Acc: 89.80%\n",
      "Epoch [1/10] | Batch [154/191] | Loss: 0.4177 | Acc: 88.80%\n",
      "Epoch [1/10] | Batch [155/191] | Loss: 0.4230 | Acc: 89.20%\n",
      "Epoch [1/10] | Batch [156/191] | Loss: 0.4163 | Acc: 89.20%\n",
      "Epoch [1/10] | Batch [157/191] | Loss: 0.3853 | Acc: 92.40%\n",
      "Epoch [1/10] | Batch [158/191] | Loss: 0.4289 | Acc: 88.40%\n",
      "Epoch [1/10] | Batch [159/191] | Loss: 0.4406 | Acc: 86.40%\n",
      "Epoch [1/10] | Batch [160/191] | Loss: 0.4177 | Acc: 88.60%\n",
      "Epoch [1/10] | Batch [161/191] | Loss: 0.4139 | Acc: 89.60%\n",
      "Epoch [1/10] | Batch [162/191] | Loss: 0.4256 | Acc: 88.60%\n",
      "Epoch [1/10] | Batch [163/191] | Loss: 0.4012 | Acc: 91.40%\n",
      "Epoch [1/10] | Batch [164/191] | Loss: 0.4129 | Acc: 89.60%\n",
      "Epoch [1/10] | Batch [165/191] | Loss: 0.4171 | Acc: 89.20%\n",
      "Epoch [1/10] | Batch [166/191] | Loss: 0.3950 | Acc: 91.80%\n",
      "Epoch [1/10] | Batch [167/191] | Loss: 0.4121 | Acc: 89.80%\n",
      "Epoch [1/10] | Batch [168/191] | Loss: 0.4118 | Acc: 88.60%\n",
      "Epoch [1/10] | Batch [169/191] | Loss: 0.4013 | Acc: 89.80%\n",
      "Epoch [1/10] | Batch [170/191] | Loss: 0.4199 | Acc: 89.00%\n",
      "Epoch [1/10] | Batch [171/191] | Loss: 0.4258 | Acc: 88.40%\n",
      "Epoch [1/10] | Batch [172/191] | Loss: 0.4013 | Acc: 91.00%\n",
      "Epoch [1/10] | Batch [173/191] | Loss: 0.4162 | Acc: 89.20%\n",
      "Epoch [1/10] | Batch [174/191] | Loss: 0.4117 | Acc: 89.40%\n",
      "Epoch [1/10] | Batch [175/191] | Loss: 0.3871 | Acc: 92.60%\n",
      "Epoch [1/10] | Batch [176/191] | Loss: 0.4067 | Acc: 90.60%\n",
      "Epoch [1/10] | Batch [177/191] | Loss: 0.3864 | Acc: 92.80%\n",
      "Epoch [1/10] | Batch [178/191] | Loss: 0.3996 | Acc: 91.40%\n",
      "Epoch [1/10] | Batch [179/191] | Loss: 0.4150 | Acc: 89.60%\n",
      "Epoch [1/10] | Batch [180/191] | Loss: 0.4069 | Acc: 90.20%\n",
      "Epoch [1/10] | Batch [181/191] | Loss: 0.3974 | Acc: 91.40%\n",
      "Epoch [1/10] | Batch [182/191] | Loss: 0.4178 | Acc: 89.60%\n",
      "Epoch [1/10] | Batch [183/191] | Loss: 0.4077 | Acc: 89.80%\n",
      "Epoch [1/10] | Batch [184/191] | Loss: 0.3806 | Acc: 93.20%\n",
      "Epoch [1/10] | Batch [185/191] | Loss: 0.3872 | Acc: 91.60%\n",
      "Epoch [1/10] | Batch [186/191] | Loss: 0.4180 | Acc: 89.20%\n",
      "Epoch [1/10] | Batch [187/191] | Loss: 0.3904 | Acc: 91.80%\n",
      "Epoch [1/10] | Batch [188/191] | Loss: 0.3946 | Acc: 91.80%\n",
      "Epoch [1/10] | Batch [189/191] | Loss: 0.4232 | Acc: 88.80%\n",
      "Epoch [1/10] | Batch [190/191] | Loss: 0.4127 | Acc: 89.60%\n",
      "Epoch [1/10] | Batch [191/191] | Loss: 0.4062 | Acc: 90.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/1024/ipykernel_22743/1531228587.py:140: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  epoch_result_df = pd.concat([epoch_result_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Train Loss: 0.5141 | Acc: 76.32% | Precision: 0.7672 | Recall: 0.7671 | F1: 0.7671\n",
      "Test  Loss: 0.4002 | Acc: 90.88% | Precision: 0.9101 | Recall: 0.9086 | F1: 0.9087\n",
      "Learning Rate: 9.78e-06\n",
      "\n",
      "Epoch [2/10] | Batch [1/191] | Loss: 0.4021 | Acc: 90.40%\n",
      "Epoch [2/10] | Batch [2/191] | Loss: 0.3974 | Acc: 91.20%\n",
      "Epoch [2/10] | Batch [3/191] | Loss: 0.3903 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [4/191] | Loss: 0.3888 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [5/191] | Loss: 0.4086 | Acc: 90.00%\n",
      "Epoch [2/10] | Batch [6/191] | Loss: 0.4147 | Acc: 89.20%\n",
      "Epoch [2/10] | Batch [7/191] | Loss: 0.3889 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [8/191] | Loss: 0.4045 | Acc: 90.80%\n",
      "Epoch [2/10] | Batch [9/191] | Loss: 0.4073 | Acc: 90.40%\n",
      "Epoch [2/10] | Batch [10/191] | Loss: 0.3953 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [11/191] | Loss: 0.3846 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [12/191] | Loss: 0.3840 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [13/191] | Loss: 0.3888 | Acc: 92.20%\n",
      "Epoch [2/10] | Batch [14/191] | Loss: 0.3891 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [15/191] | Loss: 0.3942 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [16/191] | Loss: 0.4029 | Acc: 91.00%\n",
      "Epoch [2/10] | Batch [17/191] | Loss: 0.3968 | Acc: 91.00%\n",
      "Epoch [2/10] | Batch [18/191] | Loss: 0.3881 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [19/191] | Loss: 0.4030 | Acc: 90.40%\n",
      "Epoch [2/10] | Batch [20/191] | Loss: 0.3876 | Acc: 91.80%\n",
      "Epoch [2/10] | Batch [21/191] | Loss: 0.3806 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [22/191] | Loss: 0.3897 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [23/191] | Loss: 0.3800 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [24/191] | Loss: 0.3922 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [25/191] | Loss: 0.3924 | Acc: 91.80%\n",
      "Epoch [2/10] | Batch [26/191] | Loss: 0.3991 | Acc: 90.40%\n",
      "Epoch [2/10] | Batch [27/191] | Loss: 0.3921 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [28/191] | Loss: 0.3889 | Acc: 92.20%\n",
      "Epoch [2/10] | Batch [29/191] | Loss: 0.4068 | Acc: 90.00%\n",
      "Epoch [2/10] | Batch [30/191] | Loss: 0.3889 | Acc: 92.20%\n",
      "Epoch [2/10] | Batch [31/191] | Loss: 0.3938 | Acc: 91.20%\n",
      "Epoch [2/10] | Batch [32/191] | Loss: 0.3896 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [33/191] | Loss: 0.3988 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [34/191] | Loss: 0.4017 | Acc: 90.60%\n",
      "Epoch [2/10] | Batch [35/191] | Loss: 0.3963 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [36/191] | Loss: 0.4100 | Acc: 89.40%\n",
      "Epoch [2/10] | Batch [37/191] | Loss: 0.4005 | Acc: 90.60%\n",
      "Epoch [2/10] | Batch [38/191] | Loss: 0.3934 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [39/191] | Loss: 0.3813 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [40/191] | Loss: 0.3879 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [41/191] | Loss: 0.3773 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [42/191] | Loss: 0.3873 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [43/191] | Loss: 0.3685 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [44/191] | Loss: 0.3925 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [45/191] | Loss: 0.3834 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [46/191] | Loss: 0.3834 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [47/191] | Loss: 0.4022 | Acc: 91.00%\n",
      "Epoch [2/10] | Batch [48/191] | Loss: 0.3715 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [49/191] | Loss: 0.3767 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [50/191] | Loss: 0.3882 | Acc: 92.20%\n",
      "Epoch [2/10] | Batch [51/191] | Loss: 0.3680 | Acc: 95.00%\n",
      "Epoch [2/10] | Batch [52/191] | Loss: 0.3747 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [53/191] | Loss: 0.4007 | Acc: 91.20%\n",
      "Epoch [2/10] | Batch [54/191] | Loss: 0.3737 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [55/191] | Loss: 0.3911 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [56/191] | Loss: 0.3917 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [57/191] | Loss: 0.3997 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [58/191] | Loss: 0.4069 | Acc: 90.40%\n",
      "Epoch [2/10] | Batch [59/191] | Loss: 0.3978 | Acc: 91.00%\n",
      "Epoch [2/10] | Batch [60/191] | Loss: 0.3817 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [61/191] | Loss: 0.3775 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [62/191] | Loss: 0.3769 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [63/191] | Loss: 0.4102 | Acc: 89.80%\n",
      "Epoch [2/10] | Batch [64/191] | Loss: 0.3858 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [65/191] | Loss: 0.3874 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [66/191] | Loss: 0.3653 | Acc: 94.80%\n",
      "Epoch [2/10] | Batch [67/191] | Loss: 0.3746 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [68/191] | Loss: 0.3805 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [69/191] | Loss: 0.3986 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [70/191] | Loss: 0.3943 | Acc: 91.40%\n",
      "Epoch [2/10] | Batch [71/191] | Loss: 0.3818 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [72/191] | Loss: 0.3749 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [73/191] | Loss: 0.3582 | Acc: 95.80%\n",
      "Epoch [2/10] | Batch [74/191] | Loss: 0.3735 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [75/191] | Loss: 0.3813 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [76/191] | Loss: 0.3691 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [77/191] | Loss: 0.4015 | Acc: 90.80%\n",
      "Epoch [2/10] | Batch [78/191] | Loss: 0.3675 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [79/191] | Loss: 0.3936 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [80/191] | Loss: 0.4023 | Acc: 90.80%\n",
      "Epoch [2/10] | Batch [81/191] | Loss: 0.3730 | Acc: 93.40%\n",
      "Epoch [2/10] | Batch [82/191] | Loss: 0.3849 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [83/191] | Loss: 0.3717 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [84/191] | Loss: 0.3855 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [85/191] | Loss: 0.3781 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [86/191] | Loss: 0.3943 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [87/191] | Loss: 0.3783 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [88/191] | Loss: 0.3823 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [89/191] | Loss: 0.3889 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [90/191] | Loss: 0.3843 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [91/191] | Loss: 0.3874 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [92/191] | Loss: 0.3848 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [93/191] | Loss: 0.3879 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [94/191] | Loss: 0.3690 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [95/191] | Loss: 0.3770 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [96/191] | Loss: 0.3937 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [97/191] | Loss: 0.3737 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [98/191] | Loss: 0.4007 | Acc: 90.60%\n",
      "Epoch [2/10] | Batch [99/191] | Loss: 0.3838 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [100/191] | Loss: 0.3722 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [101/191] | Loss: 0.3877 | Acc: 91.80%\n",
      "Epoch [2/10] | Batch [102/191] | Loss: 0.3683 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [103/191] | Loss: 0.3679 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [104/191] | Loss: 0.3768 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [105/191] | Loss: 0.3983 | Acc: 90.80%\n",
      "Epoch [2/10] | Batch [106/191] | Loss: 0.3611 | Acc: 95.40%\n",
      "Epoch [2/10] | Batch [107/191] | Loss: 0.3970 | Acc: 91.00%\n",
      "Epoch [2/10] | Batch [108/191] | Loss: 0.3813 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [109/191] | Loss: 0.3738 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [110/191] | Loss: 0.3787 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [111/191] | Loss: 0.3620 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [112/191] | Loss: 0.3654 | Acc: 94.80%\n",
      "Epoch [2/10] | Batch [113/191] | Loss: 0.3707 | Acc: 94.20%\n",
      "Epoch [2/10] | Batch [114/191] | Loss: 0.3745 | Acc: 93.40%\n",
      "Epoch [2/10] | Batch [115/191] | Loss: 0.3865 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [116/191] | Loss: 0.3833 | Acc: 92.20%\n",
      "Epoch [2/10] | Batch [117/191] | Loss: 0.3845 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [118/191] | Loss: 0.3622 | Acc: 95.60%\n",
      "Epoch [2/10] | Batch [119/191] | Loss: 0.3832 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [120/191] | Loss: 0.3658 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [121/191] | Loss: 0.3669 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [122/191] | Loss: 0.3990 | Acc: 90.80%\n",
      "Epoch [2/10] | Batch [123/191] | Loss: 0.3731 | Acc: 93.40%\n",
      "Epoch [2/10] | Batch [124/191] | Loss: 0.3720 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [125/191] | Loss: 0.3612 | Acc: 94.80%\n",
      "Epoch [2/10] | Batch [126/191] | Loss: 0.3816 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [127/191] | Loss: 0.3819 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [128/191] | Loss: 0.3851 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [129/191] | Loss: 0.3634 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [130/191] | Loss: 0.3890 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [131/191] | Loss: 0.3797 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [132/191] | Loss: 0.3796 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [133/191] | Loss: 0.3695 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [134/191] | Loss: 0.3824 | Acc: 92.20%\n",
      "Epoch [2/10] | Batch [135/191] | Loss: 0.3834 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [136/191] | Loss: 0.3905 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [137/191] | Loss: 0.3788 | Acc: 93.40%\n",
      "Epoch [2/10] | Batch [138/191] | Loss: 0.3752 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [139/191] | Loss: 0.3499 | Acc: 95.80%\n",
      "Epoch [2/10] | Batch [140/191] | Loss: 0.3834 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [141/191] | Loss: 0.3545 | Acc: 95.80%\n",
      "Epoch [2/10] | Batch [142/191] | Loss: 0.3779 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [143/191] | Loss: 0.3859 | Acc: 92.60%\n",
      "Epoch [2/10] | Batch [144/191] | Loss: 0.3845 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [145/191] | Loss: 0.3813 | Acc: 93.40%\n",
      "Epoch [2/10] | Batch [146/191] | Loss: 0.3900 | Acc: 91.60%\n",
      "Epoch [2/10] | Batch [147/191] | Loss: 0.3734 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [148/191] | Loss: 0.3848 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [149/191] | Loss: 0.3668 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [150/191] | Loss: 0.3810 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [151/191] | Loss: 0.3642 | Acc: 95.20%\n",
      "Epoch [2/10] | Batch [152/191] | Loss: 0.3805 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [153/191] | Loss: 0.3896 | Acc: 92.20%\n",
      "Epoch [2/10] | Batch [154/191] | Loss: 0.3789 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [155/191] | Loss: 0.3775 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [156/191] | Loss: 0.3752 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [157/191] | Loss: 0.3697 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [158/191] | Loss: 0.3659 | Acc: 94.20%\n",
      "Epoch [2/10] | Batch [159/191] | Loss: 0.3660 | Acc: 94.80%\n",
      "Epoch [2/10] | Batch [160/191] | Loss: 0.3445 | Acc: 96.40%\n",
      "Epoch [2/10] | Batch [161/191] | Loss: 0.3753 | Acc: 93.00%\n",
      "Epoch [2/10] | Batch [162/191] | Loss: 0.3786 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [163/191] | Loss: 0.3616 | Acc: 95.20%\n",
      "Epoch [2/10] | Batch [164/191] | Loss: 0.3685 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [165/191] | Loss: 0.3616 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [166/191] | Loss: 0.3808 | Acc: 93.40%\n",
      "Epoch [2/10] | Batch [167/191] | Loss: 0.3710 | Acc: 94.20%\n",
      "Epoch [2/10] | Batch [168/191] | Loss: 0.3689 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [169/191] | Loss: 0.3732 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [170/191] | Loss: 0.3810 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [171/191] | Loss: 0.3864 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [172/191] | Loss: 0.3646 | Acc: 94.60%\n",
      "Epoch [2/10] | Batch [173/191] | Loss: 0.3842 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [174/191] | Loss: 0.3826 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [175/191] | Loss: 0.3705 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [176/191] | Loss: 0.3659 | Acc: 94.20%\n",
      "Epoch [2/10] | Batch [177/191] | Loss: 0.3803 | Acc: 93.40%\n",
      "Epoch [2/10] | Batch [178/191] | Loss: 0.3829 | Acc: 92.40%\n",
      "Epoch [2/10] | Batch [179/191] | Loss: 0.3838 | Acc: 92.80%\n",
      "Epoch [2/10] | Batch [180/191] | Loss: 0.3725 | Acc: 93.60%\n",
      "Epoch [2/10] | Batch [181/191] | Loss: 0.3454 | Acc: 97.00%\n",
      "Epoch [2/10] | Batch [182/191] | Loss: 0.3787 | Acc: 93.20%\n",
      "Epoch [2/10] | Batch [183/191] | Loss: 0.3835 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [184/191] | Loss: 0.3742 | Acc: 93.80%\n",
      "Epoch [2/10] | Batch [185/191] | Loss: 0.3905 | Acc: 92.00%\n",
      "Epoch [2/10] | Batch [186/191] | Loss: 0.3769 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [187/191] | Loss: 0.3658 | Acc: 94.00%\n",
      "Epoch [2/10] | Batch [188/191] | Loss: 0.3589 | Acc: 95.20%\n",
      "Epoch [2/10] | Batch [189/191] | Loss: 0.3680 | Acc: 94.40%\n",
      "Epoch [2/10] | Batch [190/191] | Loss: 0.3950 | Acc: 91.80%\n",
      "Epoch [2/10] | Batch [191/191] | Loss: 0.3790 | Acc: 93.40%\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Train Loss: 0.3802 | Acc: 92.35% | Precision: 0.9283 | Recall: 0.9283 | F1: 0.9282\n",
      "Test  Loss: 0.3743 | Acc: 93.62% | Precision: 0.9370 | Recall: 0.9361 | F1: 0.9362\n",
      "Learning Rate: 9.14e-06\n",
      "\n",
      "Epoch [3/10] | Batch [1/191] | Loss: 0.3816 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [2/191] | Loss: 0.3747 | Acc: 93.60%\n",
      "Epoch [3/10] | Batch [3/191] | Loss: 0.3686 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [4/191] | Loss: 0.3675 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [5/191] | Loss: 0.3822 | Acc: 92.40%\n",
      "Epoch [3/10] | Batch [6/191] | Loss: 0.3611 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [7/191] | Loss: 0.3886 | Acc: 92.20%\n",
      "Epoch [3/10] | Batch [8/191] | Loss: 0.3952 | Acc: 92.00%\n",
      "Epoch [3/10] | Batch [9/191] | Loss: 0.3790 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [10/191] | Loss: 0.3599 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [11/191] | Loss: 0.3761 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [12/191] | Loss: 0.3733 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [13/191] | Loss: 0.3734 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [14/191] | Loss: 0.3597 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [15/191] | Loss: 0.3704 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [16/191] | Loss: 0.3762 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [17/191] | Loss: 0.3858 | Acc: 92.20%\n",
      "Epoch [3/10] | Batch [18/191] | Loss: 0.3655 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [19/191] | Loss: 0.3737 | Acc: 93.80%\n",
      "Epoch [3/10] | Batch [20/191] | Loss: 0.3680 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [21/191] | Loss: 0.3818 | Acc: 92.80%\n",
      "Epoch [3/10] | Batch [22/191] | Loss: 0.3555 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [23/191] | Loss: 0.3637 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [24/191] | Loss: 0.3720 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [25/191] | Loss: 0.3580 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [26/191] | Loss: 0.3653 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [27/191] | Loss: 0.3673 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [28/191] | Loss: 0.3690 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [29/191] | Loss: 0.3640 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [30/191] | Loss: 0.3545 | Acc: 96.00%\n",
      "Epoch [3/10] | Batch [31/191] | Loss: 0.3588 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [32/191] | Loss: 0.3663 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [33/191] | Loss: 0.3559 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [34/191] | Loss: 0.3622 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [35/191] | Loss: 0.3503 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [36/191] | Loss: 0.3711 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [37/191] | Loss: 0.3568 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [38/191] | Loss: 0.3694 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [39/191] | Loss: 0.3671 | Acc: 93.80%\n",
      "Epoch [3/10] | Batch [40/191] | Loss: 0.3766 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [41/191] | Loss: 0.3781 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [42/191] | Loss: 0.3599 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [43/191] | Loss: 0.3629 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [44/191] | Loss: 0.3682 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [45/191] | Loss: 0.3582 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [46/191] | Loss: 0.3583 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [47/191] | Loss: 0.3617 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [48/191] | Loss: 0.3892 | Acc: 91.60%\n",
      "Epoch [3/10] | Batch [49/191] | Loss: 0.3576 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [50/191] | Loss: 0.3597 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [51/191] | Loss: 0.3774 | Acc: 93.60%\n",
      "Epoch [3/10] | Batch [52/191] | Loss: 0.3769 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [53/191] | Loss: 0.3617 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [54/191] | Loss: 0.3758 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [55/191] | Loss: 0.3668 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [56/191] | Loss: 0.3626 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [57/191] | Loss: 0.3746 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [58/191] | Loss: 0.3651 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [59/191] | Loss: 0.3617 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [60/191] | Loss: 0.3648 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [61/191] | Loss: 0.3613 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [62/191] | Loss: 0.3606 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [63/191] | Loss: 0.3692 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [64/191] | Loss: 0.3653 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [65/191] | Loss: 0.3743 | Acc: 93.80%\n",
      "Epoch [3/10] | Batch [66/191] | Loss: 0.3728 | Acc: 93.80%\n",
      "Epoch [3/10] | Batch [67/191] | Loss: 0.3670 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [68/191] | Loss: 0.3711 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [69/191] | Loss: 0.3571 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [70/191] | Loss: 0.3771 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [71/191] | Loss: 0.3709 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [72/191] | Loss: 0.3807 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [73/191] | Loss: 0.3739 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [74/191] | Loss: 0.3684 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [75/191] | Loss: 0.3575 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [76/191] | Loss: 0.3746 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [77/191] | Loss: 0.3689 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [78/191] | Loss: 0.3600 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [79/191] | Loss: 0.3560 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [80/191] | Loss: 0.3911 | Acc: 92.20%\n",
      "Epoch [3/10] | Batch [81/191] | Loss: 0.3830 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [82/191] | Loss: 0.3606 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [83/191] | Loss: 0.3609 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [84/191] | Loss: 0.3613 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [85/191] | Loss: 0.3645 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [86/191] | Loss: 0.3807 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [87/191] | Loss: 0.3720 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [88/191] | Loss: 0.3557 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [89/191] | Loss: 0.3666 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [90/191] | Loss: 0.3687 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [91/191] | Loss: 0.3639 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [92/191] | Loss: 0.3665 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [93/191] | Loss: 0.3577 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [94/191] | Loss: 0.3802 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [95/191] | Loss: 0.3814 | Acc: 92.80%\n",
      "Epoch [3/10] | Batch [96/191] | Loss: 0.3624 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [97/191] | Loss: 0.3633 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [98/191] | Loss: 0.3631 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [99/191] | Loss: 0.3663 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [100/191] | Loss: 0.3648 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [101/191] | Loss: 0.3710 | Acc: 93.80%\n",
      "Epoch [3/10] | Batch [102/191] | Loss: 0.3701 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [103/191] | Loss: 0.3732 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [104/191] | Loss: 0.3601 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [105/191] | Loss: 0.3623 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [106/191] | Loss: 0.3635 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [107/191] | Loss: 0.3517 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [108/191] | Loss: 0.3664 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [109/191] | Loss: 0.3779 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [110/191] | Loss: 0.3481 | Acc: 96.40%\n",
      "Epoch [3/10] | Batch [111/191] | Loss: 0.3650 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [112/191] | Loss: 0.3717 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [113/191] | Loss: 0.3662 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [114/191] | Loss: 0.3565 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [115/191] | Loss: 0.3755 | Acc: 93.60%\n",
      "Epoch [3/10] | Batch [116/191] | Loss: 0.3704 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [117/191] | Loss: 0.3684 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [118/191] | Loss: 0.3519 | Acc: 96.40%\n",
      "Epoch [3/10] | Batch [119/191] | Loss: 0.3710 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [120/191] | Loss: 0.3760 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [121/191] | Loss: 0.3652 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [122/191] | Loss: 0.3602 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [123/191] | Loss: 0.3662 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [124/191] | Loss: 0.3761 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [125/191] | Loss: 0.3699 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [126/191] | Loss: 0.3827 | Acc: 92.80%\n",
      "Epoch [3/10] | Batch [127/191] | Loss: 0.3783 | Acc: 92.40%\n",
      "Epoch [3/10] | Batch [128/191] | Loss: 0.3676 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [129/191] | Loss: 0.3619 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [130/191] | Loss: 0.3594 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [131/191] | Loss: 0.3680 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [132/191] | Loss: 0.3666 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [133/191] | Loss: 0.3658 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [134/191] | Loss: 0.3512 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [135/191] | Loss: 0.3511 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [136/191] | Loss: 0.3589 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [137/191] | Loss: 0.3736 | Acc: 93.60%\n",
      "Epoch [3/10] | Batch [138/191] | Loss: 0.3641 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [139/191] | Loss: 0.3749 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [140/191] | Loss: 0.3825 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [141/191] | Loss: 0.3680 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [142/191] | Loss: 0.3591 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [143/191] | Loss: 0.3684 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [144/191] | Loss: 0.3472 | Acc: 96.60%\n",
      "Epoch [3/10] | Batch [145/191] | Loss: 0.3682 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [146/191] | Loss: 0.3736 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [147/191] | Loss: 0.3655 | Acc: 94.60%\n",
      "Epoch [3/10] | Batch [148/191] | Loss: 0.3662 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [149/191] | Loss: 0.3723 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [150/191] | Loss: 0.3745 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [151/191] | Loss: 0.3682 | Acc: 94.00%\n",
      "Epoch [3/10] | Batch [152/191] | Loss: 0.3695 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [153/191] | Loss: 0.3553 | Acc: 96.00%\n",
      "Epoch [3/10] | Batch [154/191] | Loss: 0.3831 | Acc: 92.20%\n",
      "Epoch [3/10] | Batch [155/191] | Loss: 0.3841 | Acc: 92.80%\n",
      "Epoch [3/10] | Batch [156/191] | Loss: 0.3708 | Acc: 93.40%\n",
      "Epoch [3/10] | Batch [157/191] | Loss: 0.3636 | Acc: 94.40%\n",
      "Epoch [3/10] | Batch [158/191] | Loss: 0.3614 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [159/191] | Loss: 0.3493 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [160/191] | Loss: 0.3608 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [161/191] | Loss: 0.3801 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [162/191] | Loss: 0.3794 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [163/191] | Loss: 0.3611 | Acc: 95.00%\n",
      "Epoch [3/10] | Batch [164/191] | Loss: 0.3816 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [165/191] | Loss: 0.3651 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [166/191] | Loss: 0.3648 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [167/191] | Loss: 0.3808 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [168/191] | Loss: 0.3584 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [169/191] | Loss: 0.3581 | Acc: 95.40%\n",
      "Epoch [3/10] | Batch [170/191] | Loss: 0.3621 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [171/191] | Loss: 0.3637 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [172/191] | Loss: 0.3747 | Acc: 93.60%\n",
      "Epoch [3/10] | Batch [173/191] | Loss: 0.3704 | Acc: 94.20%\n",
      "Epoch [3/10] | Batch [174/191] | Loss: 0.3738 | Acc: 93.80%\n",
      "Epoch [3/10] | Batch [175/191] | Loss: 0.3641 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [176/191] | Loss: 0.3510 | Acc: 96.20%\n",
      "Epoch [3/10] | Batch [177/191] | Loss: 0.3864 | Acc: 92.20%\n",
      "Epoch [3/10] | Batch [178/191] | Loss: 0.3576 | Acc: 95.20%\n",
      "Epoch [3/10] | Batch [179/191] | Loss: 0.3557 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [180/191] | Loss: 0.3543 | Acc: 95.80%\n",
      "Epoch [3/10] | Batch [181/191] | Loss: 0.3874 | Acc: 92.00%\n",
      "Epoch [3/10] | Batch [182/191] | Loss: 0.3795 | Acc: 93.00%\n",
      "Epoch [3/10] | Batch [183/191] | Loss: 0.3799 | Acc: 93.20%\n",
      "Epoch [3/10] | Batch [184/191] | Loss: 0.3572 | Acc: 95.60%\n",
      "Epoch [3/10] | Batch [185/191] | Loss: 0.3340 | Acc: 97.80%\n",
      "Epoch [3/10] | Batch [186/191] | Loss: 0.3461 | Acc: 97.00%\n",
      "Epoch [3/10] | Batch [187/191] | Loss: 0.3763 | Acc: 93.60%\n",
      "Epoch [3/10] | Batch [188/191] | Loss: 0.3858 | Acc: 92.60%\n",
      "Epoch [3/10] | Batch [189/191] | Loss: 0.3624 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [190/191] | Loss: 0.3641 | Acc: 94.80%\n",
      "Epoch [3/10] | Batch [191/191] | Loss: 0.3650 | Acc: 94.80%\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Train Loss: 0.3657 | Acc: 93.86% | Precision: 0.9435 | Recall: 0.9434 | F1: 0.9434\n",
      "Test  Loss: 0.3628 | Acc: 94.82% | Precision: 0.9484 | Recall: 0.9483 | F1: 0.9482\n",
      "Learning Rate: 8.15e-06\n",
      "\n",
      "Epoch [4/10] | Batch [1/191] | Loss: 0.3695 | Acc: 94.00%\n",
      "Epoch [4/10] | Batch [2/191] | Loss: 0.3672 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [3/191] | Loss: 0.3620 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [4/191] | Loss: 0.3762 | Acc: 93.80%\n",
      "Epoch [4/10] | Batch [5/191] | Loss: 0.3673 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [6/191] | Loss: 0.3607 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [7/191] | Loss: 0.3629 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [8/191] | Loss: 0.3760 | Acc: 93.20%\n",
      "Epoch [4/10] | Batch [9/191] | Loss: 0.3620 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [10/191] | Loss: 0.3657 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [11/191] | Loss: 0.3626 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [12/191] | Loss: 0.3649 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [13/191] | Loss: 0.3681 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [14/191] | Loss: 0.3637 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [15/191] | Loss: 0.3488 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [16/191] | Loss: 0.3548 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [17/191] | Loss: 0.3701 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [18/191] | Loss: 0.3720 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [19/191] | Loss: 0.3441 | Acc: 97.00%\n",
      "Epoch [4/10] | Batch [20/191] | Loss: 0.3716 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [21/191] | Loss: 0.3593 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [22/191] | Loss: 0.3557 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [23/191] | Loss: 0.3567 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [24/191] | Loss: 0.3548 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [25/191] | Loss: 0.3680 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [26/191] | Loss: 0.3572 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [27/191] | Loss: 0.3540 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [28/191] | Loss: 0.3542 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [29/191] | Loss: 0.3526 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [30/191] | Loss: 0.3517 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [31/191] | Loss: 0.3735 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [32/191] | Loss: 0.3666 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [33/191] | Loss: 0.3768 | Acc: 93.60%\n",
      "Epoch [4/10] | Batch [34/191] | Loss: 0.3753 | Acc: 93.40%\n",
      "Epoch [4/10] | Batch [35/191] | Loss: 0.3446 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [36/191] | Loss: 0.3585 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [37/191] | Loss: 0.3712 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [38/191] | Loss: 0.3792 | Acc: 92.80%\n",
      "Epoch [4/10] | Batch [39/191] | Loss: 0.3663 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [40/191] | Loss: 0.3549 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [41/191] | Loss: 0.3637 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [42/191] | Loss: 0.3701 | Acc: 93.80%\n",
      "Epoch [4/10] | Batch [43/191] | Loss: 0.3729 | Acc: 93.80%\n",
      "Epoch [4/10] | Batch [44/191] | Loss: 0.3610 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [45/191] | Loss: 0.3673 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [46/191] | Loss: 0.3496 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [47/191] | Loss: 0.3572 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [48/191] | Loss: 0.3655 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [49/191] | Loss: 0.3662 | Acc: 94.00%\n",
      "Epoch [4/10] | Batch [50/191] | Loss: 0.3739 | Acc: 93.60%\n",
      "Epoch [4/10] | Batch [51/191] | Loss: 0.3670 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [52/191] | Loss: 0.3643 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [53/191] | Loss: 0.3604 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [54/191] | Loss: 0.3538 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [55/191] | Loss: 0.3572 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [56/191] | Loss: 0.3711 | Acc: 93.80%\n",
      "Epoch [4/10] | Batch [57/191] | Loss: 0.3405 | Acc: 97.00%\n",
      "Epoch [4/10] | Batch [58/191] | Loss: 0.3623 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [59/191] | Loss: 0.3464 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [60/191] | Loss: 0.3602 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [61/191] | Loss: 0.3644 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [62/191] | Loss: 0.3475 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [63/191] | Loss: 0.3663 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [64/191] | Loss: 0.3532 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [65/191] | Loss: 0.3742 | Acc: 93.20%\n",
      "Epoch [4/10] | Batch [66/191] | Loss: 0.3695 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [67/191] | Loss: 0.3669 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [68/191] | Loss: 0.3598 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [69/191] | Loss: 0.3493 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [70/191] | Loss: 0.3516 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [71/191] | Loss: 0.3488 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [72/191] | Loss: 0.3479 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [73/191] | Loss: 0.3794 | Acc: 93.40%\n",
      "Epoch [4/10] | Batch [74/191] | Loss: 0.3374 | Acc: 97.60%\n",
      "Epoch [4/10] | Batch [75/191] | Loss: 0.3514 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [76/191] | Loss: 0.3481 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [77/191] | Loss: 0.3648 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [78/191] | Loss: 0.3546 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [79/191] | Loss: 0.3687 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [80/191] | Loss: 0.3586 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [81/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [82/191] | Loss: 0.3655 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [83/191] | Loss: 0.3565 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [84/191] | Loss: 0.3499 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [85/191] | Loss: 0.3665 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [86/191] | Loss: 0.3674 | Acc: 94.00%\n",
      "Epoch [4/10] | Batch [87/191] | Loss: 0.3890 | Acc: 91.60%\n",
      "Epoch [4/10] | Batch [88/191] | Loss: 0.3672 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [89/191] | Loss: 0.3595 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [90/191] | Loss: 0.3584 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [91/191] | Loss: 0.3540 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [92/191] | Loss: 0.3579 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [93/191] | Loss: 0.3677 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [94/191] | Loss: 0.3698 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [95/191] | Loss: 0.3738 | Acc: 93.20%\n",
      "Epoch [4/10] | Batch [96/191] | Loss: 0.3563 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [97/191] | Loss: 0.3467 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [98/191] | Loss: 0.3732 | Acc: 94.00%\n",
      "Epoch [4/10] | Batch [99/191] | Loss: 0.3483 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [100/191] | Loss: 0.3644 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [101/191] | Loss: 0.3593 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [102/191] | Loss: 0.3693 | Acc: 93.80%\n",
      "Epoch [4/10] | Batch [103/191] | Loss: 0.3547 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [104/191] | Loss: 0.3353 | Acc: 97.80%\n",
      "Epoch [4/10] | Batch [105/191] | Loss: 0.3650 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [106/191] | Loss: 0.3628 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [107/191] | Loss: 0.3881 | Acc: 91.80%\n",
      "Epoch [4/10] | Batch [108/191] | Loss: 0.3494 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [109/191] | Loss: 0.3565 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [110/191] | Loss: 0.3496 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [111/191] | Loss: 0.3565 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [112/191] | Loss: 0.3511 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [113/191] | Loss: 0.3584 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [114/191] | Loss: 0.3697 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [115/191] | Loss: 0.3634 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [116/191] | Loss: 0.3606 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [117/191] | Loss: 0.3698 | Acc: 94.00%\n",
      "Epoch [4/10] | Batch [118/191] | Loss: 0.3686 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [119/191] | Loss: 0.3619 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [120/191] | Loss: 0.3860 | Acc: 92.40%\n",
      "Epoch [4/10] | Batch [121/191] | Loss: 0.3713 | Acc: 94.00%\n",
      "Epoch [4/10] | Batch [122/191] | Loss: 0.3539 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [123/191] | Loss: 0.3535 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [124/191] | Loss: 0.3427 | Acc: 97.00%\n",
      "Epoch [4/10] | Batch [125/191] | Loss: 0.3507 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [126/191] | Loss: 0.3634 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [127/191] | Loss: 0.3457 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [128/191] | Loss: 0.3538 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [129/191] | Loss: 0.3490 | Acc: 96.60%\n",
      "Epoch [4/10] | Batch [130/191] | Loss: 0.3528 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [131/191] | Loss: 0.3518 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [132/191] | Loss: 0.3573 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [133/191] | Loss: 0.3552 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [134/191] | Loss: 0.3586 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [135/191] | Loss: 0.3494 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [136/191] | Loss: 0.3562 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [137/191] | Loss: 0.3711 | Acc: 93.60%\n",
      "Epoch [4/10] | Batch [138/191] | Loss: 0.3685 | Acc: 94.40%\n",
      "Epoch [4/10] | Batch [139/191] | Loss: 0.3530 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [140/191] | Loss: 0.3513 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [141/191] | Loss: 0.3579 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [142/191] | Loss: 0.3690 | Acc: 94.00%\n",
      "Epoch [4/10] | Batch [143/191] | Loss: 0.3611 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [144/191] | Loss: 0.3490 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [145/191] | Loss: 0.3618 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [146/191] | Loss: 0.3768 | Acc: 93.40%\n",
      "Epoch [4/10] | Batch [147/191] | Loss: 0.3528 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [148/191] | Loss: 0.3647 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [149/191] | Loss: 0.3492 | Acc: 96.40%\n",
      "Epoch [4/10] | Batch [150/191] | Loss: 0.3608 | Acc: 95.00%\n",
      "Epoch [4/10] | Batch [151/191] | Loss: 0.3607 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [152/191] | Loss: 0.3643 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [153/191] | Loss: 0.3524 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [154/191] | Loss: 0.3672 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [155/191] | Loss: 0.3542 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [156/191] | Loss: 0.3660 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [157/191] | Loss: 0.3684 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [158/191] | Loss: 0.3487 | Acc: 96.80%\n",
      "Epoch [4/10] | Batch [159/191] | Loss: 0.3574 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [160/191] | Loss: 0.3596 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [161/191] | Loss: 0.3533 | Acc: 95.40%\n",
      "Epoch [4/10] | Batch [162/191] | Loss: 0.3366 | Acc: 97.60%\n",
      "Epoch [4/10] | Batch [163/191] | Loss: 0.3675 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [164/191] | Loss: 0.3518 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [165/191] | Loss: 0.3582 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [166/191] | Loss: 0.3534 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [167/191] | Loss: 0.3508 | Acc: 96.20%\n",
      "Epoch [4/10] | Batch [168/191] | Loss: 0.3662 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [169/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [170/191] | Loss: 0.3616 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [171/191] | Loss: 0.3532 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [172/191] | Loss: 0.3553 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [173/191] | Loss: 0.3499 | Acc: 96.60%\n",
      "Epoch [4/10] | Batch [174/191] | Loss: 0.3656 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [175/191] | Loss: 0.3618 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [176/191] | Loss: 0.3637 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [177/191] | Loss: 0.3691 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [178/191] | Loss: 0.3437 | Acc: 97.00%\n",
      "Epoch [4/10] | Batch [179/191] | Loss: 0.3604 | Acc: 95.20%\n",
      "Epoch [4/10] | Batch [180/191] | Loss: 0.3859 | Acc: 92.40%\n",
      "Epoch [4/10] | Batch [181/191] | Loss: 0.3682 | Acc: 94.20%\n",
      "Epoch [4/10] | Batch [182/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [4/10] | Batch [183/191] | Loss: 0.3449 | Acc: 96.80%\n",
      "Epoch [4/10] | Batch [184/191] | Loss: 0.3460 | Acc: 96.60%\n",
      "Epoch [4/10] | Batch [185/191] | Loss: 0.3721 | Acc: 93.80%\n",
      "Epoch [4/10] | Batch [186/191] | Loss: 0.3634 | Acc: 94.80%\n",
      "Epoch [4/10] | Batch [187/191] | Loss: 0.3674 | Acc: 94.60%\n",
      "Epoch [4/10] | Batch [188/191] | Loss: 0.3547 | Acc: 95.80%\n",
      "Epoch [4/10] | Batch [189/191] | Loss: 0.3525 | Acc: 96.00%\n",
      "Epoch [4/10] | Batch [190/191] | Loss: 0.3784 | Acc: 93.20%\n",
      "Epoch [4/10] | Batch [191/191] | Loss: 0.3520 | Acc: 96.00%\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Train Loss: 0.3585 | Acc: 94.64% | Precision: 0.9514 | Recall: 0.9512 | F1: 0.9512\n",
      "Test  Loss: 0.3594 | Acc: 95.18% | Precision: 0.9520 | Recall: 0.9519 | F1: 0.9518\n",
      "Learning Rate: 6.89e-06\n",
      "\n",
      "Epoch [5/10] | Batch [1/191] | Loss: 0.3537 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [2/191] | Loss: 0.3638 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [3/191] | Loss: 0.3438 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [4/191] | Loss: 0.3690 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [5/191] | Loss: 0.3654 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [6/191] | Loss: 0.3447 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [7/191] | Loss: 0.3510 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [8/191] | Loss: 0.3525 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [9/191] | Loss: 0.3503 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [10/191] | Loss: 0.3624 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [11/191] | Loss: 0.3501 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [12/191] | Loss: 0.3583 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [13/191] | Loss: 0.3702 | Acc: 94.00%\n",
      "Epoch [5/10] | Batch [14/191] | Loss: 0.3421 | Acc: 97.20%\n",
      "Epoch [5/10] | Batch [15/191] | Loss: 0.3651 | Acc: 94.00%\n",
      "Epoch [5/10] | Batch [16/191] | Loss: 0.3494 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [17/191] | Loss: 0.3486 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [18/191] | Loss: 0.3447 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [19/191] | Loss: 0.3511 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [20/191] | Loss: 0.3556 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [21/191] | Loss: 0.3553 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [22/191] | Loss: 0.3637 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [23/191] | Loss: 0.3619 | Acc: 95.00%\n",
      "Epoch [5/10] | Batch [24/191] | Loss: 0.3486 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [25/191] | Loss: 0.3616 | Acc: 95.00%\n",
      "Epoch [5/10] | Batch [26/191] | Loss: 0.3515 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [27/191] | Loss: 0.3552 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [28/191] | Loss: 0.3645 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [29/191] | Loss: 0.3593 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [30/191] | Loss: 0.3704 | Acc: 93.60%\n",
      "Epoch [5/10] | Batch [31/191] | Loss: 0.3581 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [32/191] | Loss: 0.3722 | Acc: 93.80%\n",
      "Epoch [5/10] | Batch [33/191] | Loss: 0.3696 | Acc: 94.00%\n",
      "Epoch [5/10] | Batch [34/191] | Loss: 0.3603 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [35/191] | Loss: 0.3525 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [36/191] | Loss: 0.3608 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [37/191] | Loss: 0.3515 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [38/191] | Loss: 0.3547 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [39/191] | Loss: 0.3664 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [40/191] | Loss: 0.3546 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [41/191] | Loss: 0.3695 | Acc: 94.00%\n",
      "Epoch [5/10] | Batch [42/191] | Loss: 0.3550 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [43/191] | Loss: 0.3459 | Acc: 97.00%\n",
      "Epoch [5/10] | Batch [44/191] | Loss: 0.3515 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [45/191] | Loss: 0.3525 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [46/191] | Loss: 0.3622 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [47/191] | Loss: 0.3641 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [48/191] | Loss: 0.3519 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [49/191] | Loss: 0.3430 | Acc: 97.00%\n",
      "Epoch [5/10] | Batch [50/191] | Loss: 0.3522 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [51/191] | Loss: 0.3637 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [52/191] | Loss: 0.3475 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [53/191] | Loss: 0.3522 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [54/191] | Loss: 0.3580 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [55/191] | Loss: 0.3546 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [56/191] | Loss: 0.3662 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [57/191] | Loss: 0.3626 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [58/191] | Loss: 0.3715 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [59/191] | Loss: 0.3526 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [60/191] | Loss: 0.3576 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [61/191] | Loss: 0.3590 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [62/191] | Loss: 0.3616 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [63/191] | Loss: 0.3536 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [64/191] | Loss: 0.3517 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [65/191] | Loss: 0.3494 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [66/191] | Loss: 0.3511 | Acc: 96.60%\n",
      "Epoch [5/10] | Batch [67/191] | Loss: 0.3514 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [68/191] | Loss: 0.3516 | Acc: 96.60%\n",
      "Epoch [5/10] | Batch [69/191] | Loss: 0.3574 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [70/191] | Loss: 0.3614 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [71/191] | Loss: 0.3705 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [72/191] | Loss: 0.3527 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [73/191] | Loss: 0.3593 | Acc: 95.00%\n",
      "Epoch [5/10] | Batch [74/191] | Loss: 0.3485 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [75/191] | Loss: 0.3481 | Acc: 96.60%\n",
      "Epoch [5/10] | Batch [76/191] | Loss: 0.3484 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [77/191] | Loss: 0.3658 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [78/191] | Loss: 0.3448 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [79/191] | Loss: 0.3603 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [80/191] | Loss: 0.3479 | Acc: 96.60%\n",
      "Epoch [5/10] | Batch [81/191] | Loss: 0.3462 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [82/191] | Loss: 0.3599 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [83/191] | Loss: 0.3640 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [84/191] | Loss: 0.3452 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [85/191] | Loss: 0.3516 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [86/191] | Loss: 0.3482 | Acc: 96.60%\n",
      "Epoch [5/10] | Batch [87/191] | Loss: 0.3542 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [88/191] | Loss: 0.3693 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [89/191] | Loss: 0.3650 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [90/191] | Loss: 0.3649 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [91/191] | Loss: 0.3605 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [92/191] | Loss: 0.3622 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [93/191] | Loss: 0.3703 | Acc: 93.80%\n",
      "Epoch [5/10] | Batch [94/191] | Loss: 0.3612 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [95/191] | Loss: 0.3722 | Acc: 93.60%\n",
      "Epoch [5/10] | Batch [96/191] | Loss: 0.3485 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [97/191] | Loss: 0.3657 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [98/191] | Loss: 0.3751 | Acc: 93.60%\n",
      "Epoch [5/10] | Batch [99/191] | Loss: 0.3672 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [100/191] | Loss: 0.3712 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [101/191] | Loss: 0.3660 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [102/191] | Loss: 0.3546 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [103/191] | Loss: 0.3575 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [104/191] | Loss: 0.3476 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [105/191] | Loss: 0.3646 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [106/191] | Loss: 0.3607 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [107/191] | Loss: 0.3648 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [108/191] | Loss: 0.3539 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [109/191] | Loss: 0.3544 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [110/191] | Loss: 0.3520 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [111/191] | Loss: 0.3681 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [112/191] | Loss: 0.3633 | Acc: 95.00%\n",
      "Epoch [5/10] | Batch [113/191] | Loss: 0.3423 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [114/191] | Loss: 0.3531 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [115/191] | Loss: 0.3531 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [116/191] | Loss: 0.3501 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [117/191] | Loss: 0.3674 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [118/191] | Loss: 0.3420 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [119/191] | Loss: 0.3403 | Acc: 97.40%\n",
      "Epoch [5/10] | Batch [120/191] | Loss: 0.3647 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [121/191] | Loss: 0.3570 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [122/191] | Loss: 0.3675 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [123/191] | Loss: 0.3442 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [124/191] | Loss: 0.3742 | Acc: 93.80%\n",
      "Epoch [5/10] | Batch [125/191] | Loss: 0.3651 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [126/191] | Loss: 0.3700 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [127/191] | Loss: 0.3718 | Acc: 93.80%\n",
      "Epoch [5/10] | Batch [128/191] | Loss: 0.3538 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [129/191] | Loss: 0.3652 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [130/191] | Loss: 0.3513 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [131/191] | Loss: 0.3596 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [132/191] | Loss: 0.3433 | Acc: 97.00%\n",
      "Epoch [5/10] | Batch [133/191] | Loss: 0.3618 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [134/191] | Loss: 0.3483 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [135/191] | Loss: 0.3546 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [136/191] | Loss: 0.3507 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [137/191] | Loss: 0.3625 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [138/191] | Loss: 0.3560 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [139/191] | Loss: 0.3546 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [140/191] | Loss: 0.3576 | Acc: 95.00%\n",
      "Epoch [5/10] | Batch [141/191] | Loss: 0.3636 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [142/191] | Loss: 0.3527 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [143/191] | Loss: 0.3598 | Acc: 95.00%\n",
      "Epoch [5/10] | Batch [144/191] | Loss: 0.3451 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [145/191] | Loss: 0.3436 | Acc: 97.00%\n",
      "Epoch [5/10] | Batch [146/191] | Loss: 0.3489 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [147/191] | Loss: 0.3634 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [148/191] | Loss: 0.3652 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [149/191] | Loss: 0.3708 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [150/191] | Loss: 0.3649 | Acc: 94.40%\n",
      "Epoch [5/10] | Batch [151/191] | Loss: 0.3473 | Acc: 97.00%\n",
      "Epoch [5/10] | Batch [152/191] | Loss: 0.3608 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [153/191] | Loss: 0.3583 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [154/191] | Loss: 0.3441 | Acc: 97.00%\n",
      "Epoch [5/10] | Batch [155/191] | Loss: 0.3625 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [156/191] | Loss: 0.3578 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [157/191] | Loss: 0.3501 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [158/191] | Loss: 0.3523 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [159/191] | Loss: 0.3463 | Acc: 96.20%\n",
      "Epoch [5/10] | Batch [160/191] | Loss: 0.3602 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [161/191] | Loss: 0.3615 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [162/191] | Loss: 0.3503 | Acc: 96.00%\n",
      "Epoch [5/10] | Batch [163/191] | Loss: 0.3576 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [164/191] | Loss: 0.3799 | Acc: 93.20%\n",
      "Epoch [5/10] | Batch [165/191] | Loss: 0.3531 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [166/191] | Loss: 0.3673 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [167/191] | Loss: 0.3343 | Acc: 97.80%\n",
      "Epoch [5/10] | Batch [168/191] | Loss: 0.3391 | Acc: 97.00%\n",
      "Epoch [5/10] | Batch [169/191] | Loss: 0.3667 | Acc: 94.20%\n",
      "Epoch [5/10] | Batch [170/191] | Loss: 0.3616 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [171/191] | Loss: 0.3531 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [172/191] | Loss: 0.3464 | Acc: 96.60%\n",
      "Epoch [5/10] | Batch [173/191] | Loss: 0.3522 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [174/191] | Loss: 0.3454 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [175/191] | Loss: 0.3567 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [176/191] | Loss: 0.3500 | Acc: 96.40%\n",
      "Epoch [5/10] | Batch [177/191] | Loss: 0.3448 | Acc: 96.80%\n",
      "Epoch [5/10] | Batch [178/191] | Loss: 0.3546 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [179/191] | Loss: 0.3757 | Acc: 93.60%\n",
      "Epoch [5/10] | Batch [180/191] | Loss: 0.3605 | Acc: 95.20%\n",
      "Epoch [5/10] | Batch [181/191] | Loss: 0.3643 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [182/191] | Loss: 0.3526 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [183/191] | Loss: 0.3792 | Acc: 93.40%\n",
      "Epoch [5/10] | Batch [184/191] | Loss: 0.3508 | Acc: 95.80%\n",
      "Epoch [5/10] | Batch [185/191] | Loss: 0.3746 | Acc: 93.80%\n",
      "Epoch [5/10] | Batch [186/191] | Loss: 0.3557 | Acc: 95.60%\n",
      "Epoch [5/10] | Batch [187/191] | Loss: 0.3605 | Acc: 95.40%\n",
      "Epoch [5/10] | Batch [188/191] | Loss: 0.3589 | Acc: 94.80%\n",
      "Epoch [5/10] | Batch [189/191] | Loss: 0.3639 | Acc: 94.60%\n",
      "Epoch [5/10] | Batch [190/191] | Loss: 0.3359 | Acc: 97.60%\n",
      "Epoch [5/10] | Batch [191/191] | Loss: 0.3556 | Acc: 96.00%\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Train Loss: 0.3553 | Acc: 94.98% | Precision: 0.9549 | Recall: 0.9546 | F1: 0.9546\n",
      "Test  Loss: 0.3589 | Acc: 95.27% | Precision: 0.9527 | Recall: 0.9527 | F1: 0.9527\n",
      "Learning Rate: 5.50e-06\n",
      "\n",
      "Epoch [6/10] | Batch [1/191] | Loss: 0.3487 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [2/191] | Loss: 0.3534 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [3/191] | Loss: 0.3618 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [4/191] | Loss: 0.3520 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [5/191] | Loss: 0.3610 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [6/191] | Loss: 0.3609 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [7/191] | Loss: 0.3585 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [8/191] | Loss: 0.3562 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [9/191] | Loss: 0.3605 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [10/191] | Loss: 0.3427 | Acc: 97.00%\n",
      "Epoch [6/10] | Batch [11/191] | Loss: 0.3527 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [12/191] | Loss: 0.3563 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [13/191] | Loss: 0.3651 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [14/191] | Loss: 0.3612 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [15/191] | Loss: 0.3544 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [16/191] | Loss: 0.3582 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [17/191] | Loss: 0.3501 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [18/191] | Loss: 0.3643 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [19/191] | Loss: 0.3514 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [20/191] | Loss: 0.3482 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [21/191] | Loss: 0.3506 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [22/191] | Loss: 0.3554 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [23/191] | Loss: 0.3589 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [24/191] | Loss: 0.3496 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [25/191] | Loss: 0.3427 | Acc: 97.00%\n",
      "Epoch [6/10] | Batch [26/191] | Loss: 0.3571 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [27/191] | Loss: 0.3597 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [28/191] | Loss: 0.3571 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [29/191] | Loss: 0.3636 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [30/191] | Loss: 0.3608 | Acc: 94.60%\n",
      "Epoch [6/10] | Batch [31/191] | Loss: 0.3547 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [32/191] | Loss: 0.3582 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [33/191] | Loss: 0.3473 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [34/191] | Loss: 0.3518 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [35/191] | Loss: 0.3613 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [36/191] | Loss: 0.3555 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [37/191] | Loss: 0.3531 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [38/191] | Loss: 0.3456 | Acc: 97.00%\n",
      "Epoch [6/10] | Batch [39/191] | Loss: 0.3521 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [40/191] | Loss: 0.3336 | Acc: 98.00%\n",
      "Epoch [6/10] | Batch [41/191] | Loss: 0.3596 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [42/191] | Loss: 0.3555 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [43/191] | Loss: 0.3480 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [44/191] | Loss: 0.3620 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [45/191] | Loss: 0.3562 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [46/191] | Loss: 0.3526 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [47/191] | Loss: 0.3511 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [48/191] | Loss: 0.3511 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [49/191] | Loss: 0.3520 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [50/191] | Loss: 0.3605 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [51/191] | Loss: 0.3662 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [52/191] | Loss: 0.3453 | Acc: 97.00%\n",
      "Epoch [6/10] | Batch [53/191] | Loss: 0.3414 | Acc: 97.00%\n",
      "Epoch [6/10] | Batch [54/191] | Loss: 0.3594 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [55/191] | Loss: 0.3502 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [56/191] | Loss: 0.3487 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [57/191] | Loss: 0.3545 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [58/191] | Loss: 0.3738 | Acc: 93.80%\n",
      "Epoch [6/10] | Batch [59/191] | Loss: 0.3437 | Acc: 96.80%\n",
      "Epoch [6/10] | Batch [60/191] | Loss: 0.3694 | Acc: 94.60%\n",
      "Epoch [6/10] | Batch [61/191] | Loss: 0.3476 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [62/191] | Loss: 0.3512 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [63/191] | Loss: 0.3578 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [64/191] | Loss: 0.3569 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [65/191] | Loss: 0.3488 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [66/191] | Loss: 0.3438 | Acc: 97.00%\n",
      "Epoch [6/10] | Batch [67/191] | Loss: 0.3631 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [68/191] | Loss: 0.3489 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [69/191] | Loss: 0.3449 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [70/191] | Loss: 0.3506 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [71/191] | Loss: 0.3536 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [72/191] | Loss: 0.3604 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [73/191] | Loss: 0.3657 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [74/191] | Loss: 0.3535 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [75/191] | Loss: 0.3422 | Acc: 97.00%\n",
      "Epoch [6/10] | Batch [76/191] | Loss: 0.3590 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [77/191] | Loss: 0.3640 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [78/191] | Loss: 0.3620 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [79/191] | Loss: 0.3522 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [80/191] | Loss: 0.3654 | Acc: 94.60%\n",
      "Epoch [6/10] | Batch [81/191] | Loss: 0.3415 | Acc: 97.20%\n",
      "Epoch [6/10] | Batch [82/191] | Loss: 0.3490 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [83/191] | Loss: 0.3563 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [84/191] | Loss: 0.3475 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [85/191] | Loss: 0.3541 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [86/191] | Loss: 0.3671 | Acc: 94.60%\n",
      "Epoch [6/10] | Batch [87/191] | Loss: 0.3453 | Acc: 96.80%\n",
      "Epoch [6/10] | Batch [88/191] | Loss: 0.3560 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [89/191] | Loss: 0.3582 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [90/191] | Loss: 0.3449 | Acc: 96.80%\n",
      "Epoch [6/10] | Batch [91/191] | Loss: 0.3565 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [92/191] | Loss: 0.3583 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [93/191] | Loss: 0.3519 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [94/191] | Loss: 0.3682 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [95/191] | Loss: 0.3508 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [96/191] | Loss: 0.3553 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [97/191] | Loss: 0.3565 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [98/191] | Loss: 0.3587 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [99/191] | Loss: 0.3802 | Acc: 93.20%\n",
      "Epoch [6/10] | Batch [100/191] | Loss: 0.3603 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [101/191] | Loss: 0.3392 | Acc: 97.20%\n",
      "Epoch [6/10] | Batch [102/191] | Loss: 0.3596 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [103/191] | Loss: 0.3616 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [104/191] | Loss: 0.3642 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [105/191] | Loss: 0.3579 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [106/191] | Loss: 0.3550 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [107/191] | Loss: 0.3534 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [108/191] | Loss: 0.3612 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [109/191] | Loss: 0.3599 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [110/191] | Loss: 0.3482 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [111/191] | Loss: 0.3541 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [112/191] | Loss: 0.3557 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [113/191] | Loss: 0.3606 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [114/191] | Loss: 0.3743 | Acc: 94.00%\n",
      "Epoch [6/10] | Batch [115/191] | Loss: 0.3654 | Acc: 94.20%\n",
      "Epoch [6/10] | Batch [116/191] | Loss: 0.3467 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [117/191] | Loss: 0.3484 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [118/191] | Loss: 0.3609 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [119/191] | Loss: 0.3651 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [120/191] | Loss: 0.3571 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [121/191] | Loss: 0.3561 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [122/191] | Loss: 0.3446 | Acc: 96.80%\n",
      "Epoch [6/10] | Batch [123/191] | Loss: 0.3675 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [124/191] | Loss: 0.3566 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [125/191] | Loss: 0.3551 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [126/191] | Loss: 0.3614 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [127/191] | Loss: 0.3536 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [128/191] | Loss: 0.3657 | Acc: 94.60%\n",
      "Epoch [6/10] | Batch [129/191] | Loss: 0.3465 | Acc: 96.80%\n",
      "Epoch [6/10] | Batch [130/191] | Loss: 0.3515 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [131/191] | Loss: 0.3391 | Acc: 97.40%\n",
      "Epoch [6/10] | Batch [132/191] | Loss: 0.3470 | Acc: 96.80%\n",
      "Epoch [6/10] | Batch [133/191] | Loss: 0.3542 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [134/191] | Loss: 0.3542 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [135/191] | Loss: 0.3497 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [136/191] | Loss: 0.3457 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [137/191] | Loss: 0.3602 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [138/191] | Loss: 0.3615 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [139/191] | Loss: 0.3525 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [140/191] | Loss: 0.3792 | Acc: 93.20%\n",
      "Epoch [6/10] | Batch [141/191] | Loss: 0.3702 | Acc: 94.00%\n",
      "Epoch [6/10] | Batch [142/191] | Loss: 0.3639 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [143/191] | Loss: 0.3607 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [144/191] | Loss: 0.3634 | Acc: 94.60%\n",
      "Epoch [6/10] | Batch [145/191] | Loss: 0.3639 | Acc: 94.80%\n",
      "Epoch [6/10] | Batch [146/191] | Loss: 0.3605 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [147/191] | Loss: 0.3501 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [148/191] | Loss: 0.3677 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [149/191] | Loss: 0.3563 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [150/191] | Loss: 0.3688 | Acc: 94.00%\n",
      "Epoch [6/10] | Batch [151/191] | Loss: 0.3688 | Acc: 94.20%\n",
      "Epoch [6/10] | Batch [152/191] | Loss: 0.3514 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [153/191] | Loss: 0.3568 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [154/191] | Loss: 0.3474 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [155/191] | Loss: 0.3471 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [156/191] | Loss: 0.3514 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [157/191] | Loss: 0.3648 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [158/191] | Loss: 0.3627 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [159/191] | Loss: 0.3688 | Acc: 94.00%\n",
      "Epoch [6/10] | Batch [160/191] | Loss: 0.3594 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [161/191] | Loss: 0.3414 | Acc: 97.40%\n",
      "Epoch [6/10] | Batch [162/191] | Loss: 0.3556 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [163/191] | Loss: 0.3602 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [164/191] | Loss: 0.3513 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [165/191] | Loss: 0.3550 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [166/191] | Loss: 0.3463 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [167/191] | Loss: 0.3530 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [168/191] | Loss: 0.3557 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [169/191] | Loss: 0.3544 | Acc: 95.60%\n",
      "Epoch [6/10] | Batch [170/191] | Loss: 0.3469 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [171/191] | Loss: 0.3530 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [172/191] | Loss: 0.3458 | Acc: 96.60%\n",
      "Epoch [6/10] | Batch [173/191] | Loss: 0.3531 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [174/191] | Loss: 0.3528 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [175/191] | Loss: 0.3680 | Acc: 94.60%\n",
      "Epoch [6/10] | Batch [176/191] | Loss: 0.3757 | Acc: 93.60%\n",
      "Epoch [6/10] | Batch [177/191] | Loss: 0.3509 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [178/191] | Loss: 0.3582 | Acc: 95.40%\n",
      "Epoch [6/10] | Batch [179/191] | Loss: 0.3540 | Acc: 96.00%\n",
      "Epoch [6/10] | Batch [180/191] | Loss: 0.3410 | Acc: 97.20%\n",
      "Epoch [6/10] | Batch [181/191] | Loss: 0.3658 | Acc: 94.40%\n",
      "Epoch [6/10] | Batch [182/191] | Loss: 0.3622 | Acc: 95.00%\n",
      "Epoch [6/10] | Batch [183/191] | Loss: 0.3508 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [184/191] | Loss: 0.3526 | Acc: 95.80%\n",
      "Epoch [6/10] | Batch [185/191] | Loss: 0.3470 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [186/191] | Loss: 0.3488 | Acc: 96.40%\n",
      "Epoch [6/10] | Batch [187/191] | Loss: 0.3567 | Acc: 95.20%\n",
      "Epoch [6/10] | Batch [188/191] | Loss: 0.3512 | Acc: 96.20%\n",
      "Epoch [6/10] | Batch [189/191] | Loss: 0.3464 | Acc: 96.80%\n",
      "Epoch [6/10] | Batch [190/191] | Loss: 0.3798 | Acc: 93.20%\n",
      "Epoch [6/10] | Batch [191/191] | Loss: 0.3608 | Acc: 95.60%\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Train Loss: 0.3539 | Acc: 95.14% | Precision: 0.9565 | Recall: 0.9562 | F1: 0.9562\n",
      "Test  Loss: 0.3603 | Acc: 95.14% | Precision: 0.9521 | Recall: 0.9515 | F1: 0.9514\n",
      "Learning Rate: 4.11e-06\n",
      "\n",
      "Epoch [7/10] | Batch [1/191] | Loss: 0.3590 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [2/191] | Loss: 0.3430 | Acc: 97.00%\n",
      "Epoch [7/10] | Batch [3/191] | Loss: 0.3653 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [4/191] | Loss: 0.3549 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [5/191] | Loss: 0.3489 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [6/191] | Loss: 0.3465 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [7/191] | Loss: 0.3620 | Acc: 95.00%\n",
      "Epoch [7/10] | Batch [8/191] | Loss: 0.3495 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [9/191] | Loss: 0.3583 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [10/191] | Loss: 0.3483 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [11/191] | Loss: 0.3628 | Acc: 95.00%\n",
      "Epoch [7/10] | Batch [12/191] | Loss: 0.3480 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [13/191] | Loss: 0.3678 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [14/191] | Loss: 0.3543 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [15/191] | Loss: 0.3571 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [16/191] | Loss: 0.3709 | Acc: 93.60%\n",
      "Epoch [7/10] | Batch [17/191] | Loss: 0.3536 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [18/191] | Loss: 0.3703 | Acc: 94.20%\n",
      "Epoch [7/10] | Batch [19/191] | Loss: 0.3470 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [20/191] | Loss: 0.3539 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [21/191] | Loss: 0.3569 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [22/191] | Loss: 0.3534 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [23/191] | Loss: 0.3558 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [24/191] | Loss: 0.3385 | Acc: 97.40%\n",
      "Epoch [7/10] | Batch [25/191] | Loss: 0.3446 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [26/191] | Loss: 0.3436 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [27/191] | Loss: 0.3490 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [28/191] | Loss: 0.3524 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [29/191] | Loss: 0.3558 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [30/191] | Loss: 0.3467 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [31/191] | Loss: 0.3483 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [32/191] | Loss: 0.3481 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [33/191] | Loss: 0.3555 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [34/191] | Loss: 0.3687 | Acc: 94.20%\n",
      "Epoch [7/10] | Batch [35/191] | Loss: 0.3504 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [36/191] | Loss: 0.3629 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [37/191] | Loss: 0.3517 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [38/191] | Loss: 0.3494 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [39/191] | Loss: 0.3721 | Acc: 94.00%\n",
      "Epoch [7/10] | Batch [40/191] | Loss: 0.3508 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [41/191] | Loss: 0.3576 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [42/191] | Loss: 0.3626 | Acc: 95.00%\n",
      "Epoch [7/10] | Batch [43/191] | Loss: 0.3404 | Acc: 97.20%\n",
      "Epoch [7/10] | Batch [44/191] | Loss: 0.3541 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [45/191] | Loss: 0.3562 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [46/191] | Loss: 0.3523 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [47/191] | Loss: 0.3576 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [48/191] | Loss: 0.3567 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [49/191] | Loss: 0.3523 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [50/191] | Loss: 0.3594 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [51/191] | Loss: 0.3538 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [52/191] | Loss: 0.3527 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [53/191] | Loss: 0.3455 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [54/191] | Loss: 0.3502 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [55/191] | Loss: 0.3494 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [56/191] | Loss: 0.3535 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [57/191] | Loss: 0.3454 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [58/191] | Loss: 0.3524 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [59/191] | Loss: 0.3548 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [60/191] | Loss: 0.3543 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [61/191] | Loss: 0.3643 | Acc: 95.00%\n",
      "Epoch [7/10] | Batch [62/191] | Loss: 0.3547 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [63/191] | Loss: 0.3655 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [64/191] | Loss: 0.3524 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [65/191] | Loss: 0.3422 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [66/191] | Loss: 0.3410 | Acc: 97.20%\n",
      "Epoch [7/10] | Batch [67/191] | Loss: 0.3523 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [68/191] | Loss: 0.3623 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [69/191] | Loss: 0.3507 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [70/191] | Loss: 0.3552 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [71/191] | Loss: 0.3661 | Acc: 94.80%\n",
      "Epoch [7/10] | Batch [72/191] | Loss: 0.3543 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [73/191] | Loss: 0.3602 | Acc: 95.00%\n",
      "Epoch [7/10] | Batch [74/191] | Loss: 0.3475 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [75/191] | Loss: 0.3524 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [76/191] | Loss: 0.3646 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [77/191] | Loss: 0.3589 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [78/191] | Loss: 0.3755 | Acc: 93.80%\n",
      "Epoch [7/10] | Batch [79/191] | Loss: 0.3724 | Acc: 93.80%\n",
      "Epoch [7/10] | Batch [80/191] | Loss: 0.3616 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [81/191] | Loss: 0.3689 | Acc: 94.00%\n",
      "Epoch [7/10] | Batch [82/191] | Loss: 0.3400 | Acc: 97.00%\n",
      "Epoch [7/10] | Batch [83/191] | Loss: 0.3440 | Acc: 97.00%\n",
      "Epoch [7/10] | Batch [84/191] | Loss: 0.3479 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [85/191] | Loss: 0.3608 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [86/191] | Loss: 0.3448 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [87/191] | Loss: 0.3720 | Acc: 94.40%\n",
      "Epoch [7/10] | Batch [88/191] | Loss: 0.3471 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [89/191] | Loss: 0.3668 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [90/191] | Loss: 0.3464 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [91/191] | Loss: 0.3691 | Acc: 94.40%\n",
      "Epoch [7/10] | Batch [92/191] | Loss: 0.3661 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [93/191] | Loss: 0.3570 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [94/191] | Loss: 0.3320 | Acc: 98.20%\n",
      "Epoch [7/10] | Batch [95/191] | Loss: 0.3454 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [96/191] | Loss: 0.3661 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [97/191] | Loss: 0.3455 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [98/191] | Loss: 0.3597 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [99/191] | Loss: 0.3536 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [100/191] | Loss: 0.3546 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [101/191] | Loss: 0.3462 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [102/191] | Loss: 0.3701 | Acc: 94.00%\n",
      "Epoch [7/10] | Batch [103/191] | Loss: 0.3500 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [104/191] | Loss: 0.3517 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [105/191] | Loss: 0.3490 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [106/191] | Loss: 0.3513 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [107/191] | Loss: 0.3448 | Acc: 97.00%\n",
      "Epoch [7/10] | Batch [108/191] | Loss: 0.3468 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [109/191] | Loss: 0.3495 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [110/191] | Loss: 0.3682 | Acc: 94.20%\n",
      "Epoch [7/10] | Batch [111/191] | Loss: 0.3540 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [112/191] | Loss: 0.3540 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [113/191] | Loss: 0.3703 | Acc: 94.40%\n",
      "Epoch [7/10] | Batch [114/191] | Loss: 0.3522 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [115/191] | Loss: 0.3498 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [116/191] | Loss: 0.3560 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [117/191] | Loss: 0.3468 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [118/191] | Loss: 0.3471 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [119/191] | Loss: 0.3479 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [120/191] | Loss: 0.3552 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [121/191] | Loss: 0.3382 | Acc: 97.40%\n",
      "Epoch [7/10] | Batch [122/191] | Loss: 0.3579 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [123/191] | Loss: 0.3558 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [124/191] | Loss: 0.3723 | Acc: 94.00%\n",
      "Epoch [7/10] | Batch [125/191] | Loss: 0.3561 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [126/191] | Loss: 0.3593 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [127/191] | Loss: 0.3583 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [128/191] | Loss: 0.3596 | Acc: 94.80%\n",
      "Epoch [7/10] | Batch [129/191] | Loss: 0.3449 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [130/191] | Loss: 0.3602 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [131/191] | Loss: 0.3621 | Acc: 94.80%\n",
      "Epoch [7/10] | Batch [132/191] | Loss: 0.3826 | Acc: 93.00%\n",
      "Epoch [7/10] | Batch [133/191] | Loss: 0.3601 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [134/191] | Loss: 0.3453 | Acc: 97.00%\n",
      "Epoch [7/10] | Batch [135/191] | Loss: 0.3571 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [136/191] | Loss: 0.3490 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [137/191] | Loss: 0.3594 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [138/191] | Loss: 0.3424 | Acc: 97.00%\n",
      "Epoch [7/10] | Batch [139/191] | Loss: 0.3503 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [140/191] | Loss: 0.3382 | Acc: 97.40%\n",
      "Epoch [7/10] | Batch [141/191] | Loss: 0.3503 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [142/191] | Loss: 0.3567 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [143/191] | Loss: 0.3553 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [144/191] | Loss: 0.3767 | Acc: 93.80%\n",
      "Epoch [7/10] | Batch [145/191] | Loss: 0.3506 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [146/191] | Loss: 0.3592 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [147/191] | Loss: 0.3526 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [148/191] | Loss: 0.3554 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [149/191] | Loss: 0.3597 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [150/191] | Loss: 0.3439 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [151/191] | Loss: 0.3555 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [152/191] | Loss: 0.3526 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [153/191] | Loss: 0.3519 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [154/191] | Loss: 0.3743 | Acc: 94.00%\n",
      "Epoch [7/10] | Batch [155/191] | Loss: 0.3577 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [156/191] | Loss: 0.3483 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [157/191] | Loss: 0.3686 | Acc: 94.80%\n",
      "Epoch [7/10] | Batch [158/191] | Loss: 0.3430 | Acc: 97.20%\n",
      "Epoch [7/10] | Batch [159/191] | Loss: 0.3503 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [160/191] | Loss: 0.3568 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [161/191] | Loss: 0.3541 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [162/191] | Loss: 0.3382 | Acc: 97.60%\n",
      "Epoch [7/10] | Batch [163/191] | Loss: 0.3600 | Acc: 95.20%\n",
      "Epoch [7/10] | Batch [164/191] | Loss: 0.3494 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [165/191] | Loss: 0.3555 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [166/191] | Loss: 0.3458 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [167/191] | Loss: 0.3563 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [168/191] | Loss: 0.3650 | Acc: 94.60%\n",
      "Epoch [7/10] | Batch [169/191] | Loss: 0.3596 | Acc: 95.40%\n",
      "Epoch [7/10] | Batch [170/191] | Loss: 0.3500 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [171/191] | Loss: 0.3518 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [172/191] | Loss: 0.3513 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [173/191] | Loss: 0.3622 | Acc: 94.80%\n",
      "Epoch [7/10] | Batch [174/191] | Loss: 0.3491 | Acc: 96.40%\n",
      "Epoch [7/10] | Batch [175/191] | Loss: 0.3476 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [176/191] | Loss: 0.3484 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [177/191] | Loss: 0.3611 | Acc: 95.00%\n",
      "Epoch [7/10] | Batch [178/191] | Loss: 0.3480 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [179/191] | Loss: 0.3575 | Acc: 95.60%\n",
      "Epoch [7/10] | Batch [180/191] | Loss: 0.3467 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [181/191] | Loss: 0.3441 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [182/191] | Loss: 0.3506 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [183/191] | Loss: 0.3472 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [184/191] | Loss: 0.3727 | Acc: 94.00%\n",
      "Epoch [7/10] | Batch [185/191] | Loss: 0.3501 | Acc: 95.80%\n",
      "Epoch [7/10] | Batch [186/191] | Loss: 0.3470 | Acc: 96.80%\n",
      "Epoch [7/10] | Batch [187/191] | Loss: 0.3530 | Acc: 96.00%\n",
      "Epoch [7/10] | Batch [188/191] | Loss: 0.3488 | Acc: 96.20%\n",
      "Epoch [7/10] | Batch [189/191] | Loss: 0.3467 | Acc: 96.60%\n",
      "Epoch [7/10] | Batch [190/191] | Loss: 0.3706 | Acc: 93.80%\n",
      "Epoch [7/10] | Batch [191/191] | Loss: 0.3544 | Acc: 96.00%\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Train Loss: 0.3527 | Acc: 95.28% | Precision: 0.9579 | Recall: 0.9576 | F1: 0.9576\n",
      "Test  Loss: 0.3573 | Acc: 95.50% | Precision: 0.9553 | Recall: 0.9550 | F1: 0.9549\n",
      "Learning Rate: 2.85e-06\n",
      "\n",
      "Epoch [8/10] | Batch [1/191] | Loss: 0.3607 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [2/191] | Loss: 0.3533 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [3/191] | Loss: 0.3509 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [4/191] | Loss: 0.3399 | Acc: 97.40%\n",
      "Epoch [8/10] | Batch [5/191] | Loss: 0.3428 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [6/191] | Loss: 0.3513 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [7/191] | Loss: 0.3498 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [8/191] | Loss: 0.3614 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [9/191] | Loss: 0.3510 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [10/191] | Loss: 0.3476 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [11/191] | Loss: 0.3536 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [12/191] | Loss: 0.3607 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [13/191] | Loss: 0.3462 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [14/191] | Loss: 0.3491 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [15/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [16/191] | Loss: 0.3533 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [17/191] | Loss: 0.3429 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [18/191] | Loss: 0.3710 | Acc: 94.20%\n",
      "Epoch [8/10] | Batch [19/191] | Loss: 0.3632 | Acc: 94.60%\n",
      "Epoch [8/10] | Batch [20/191] | Loss: 0.3501 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [21/191] | Loss: 0.3577 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [22/191] | Loss: 0.3533 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [23/191] | Loss: 0.3441 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [24/191] | Loss: 0.3490 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [25/191] | Loss: 0.3644 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [26/191] | Loss: 0.3451 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [27/191] | Loss: 0.3581 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [28/191] | Loss: 0.3533 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [29/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [30/191] | Loss: 0.3511 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [31/191] | Loss: 0.3452 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [32/191] | Loss: 0.3464 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [33/191] | Loss: 0.3430 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [34/191] | Loss: 0.3493 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [35/191] | Loss: 0.3426 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [36/191] | Loss: 0.3565 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [37/191] | Loss: 0.3490 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [38/191] | Loss: 0.3475 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [39/191] | Loss: 0.3556 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [40/191] | Loss: 0.3565 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [41/191] | Loss: 0.3646 | Acc: 94.60%\n",
      "Epoch [8/10] | Batch [42/191] | Loss: 0.3474 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [43/191] | Loss: 0.3544 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [44/191] | Loss: 0.3378 | Acc: 97.60%\n",
      "Epoch [8/10] | Batch [45/191] | Loss: 0.3669 | Acc: 94.40%\n",
      "Epoch [8/10] | Batch [46/191] | Loss: 0.3410 | Acc: 97.20%\n",
      "Epoch [8/10] | Batch [47/191] | Loss: 0.3695 | Acc: 94.20%\n",
      "Epoch [8/10] | Batch [48/191] | Loss: 0.3593 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [49/191] | Loss: 0.3387 | Acc: 97.60%\n",
      "Epoch [8/10] | Batch [50/191] | Loss: 0.3468 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [51/191] | Loss: 0.3486 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [52/191] | Loss: 0.3543 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [53/191] | Loss: 0.3610 | Acc: 94.60%\n",
      "Epoch [8/10] | Batch [54/191] | Loss: 0.3623 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [55/191] | Loss: 0.3613 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [56/191] | Loss: 0.3516 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [57/191] | Loss: 0.3542 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [58/191] | Loss: 0.3562 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [59/191] | Loss: 0.3399 | Acc: 97.40%\n",
      "Epoch [8/10] | Batch [60/191] | Loss: 0.3735 | Acc: 94.20%\n",
      "Epoch [8/10] | Batch [61/191] | Loss: 0.3385 | Acc: 97.40%\n",
      "Epoch [8/10] | Batch [62/191] | Loss: 0.3383 | Acc: 97.80%\n",
      "Epoch [8/10] | Batch [63/191] | Loss: 0.3519 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [64/191] | Loss: 0.3647 | Acc: 94.60%\n",
      "Epoch [8/10] | Batch [65/191] | Loss: 0.3449 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [66/191] | Loss: 0.3585 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [67/191] | Loss: 0.3558 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [68/191] | Loss: 0.3568 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [69/191] | Loss: 0.3485 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [70/191] | Loss: 0.3610 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [71/191] | Loss: 0.3530 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [72/191] | Loss: 0.3710 | Acc: 94.20%\n",
      "Epoch [8/10] | Batch [73/191] | Loss: 0.3389 | Acc: 97.40%\n",
      "Epoch [8/10] | Batch [74/191] | Loss: 0.3472 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [75/191] | Loss: 0.3552 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [76/191] | Loss: 0.3563 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [77/191] | Loss: 0.3605 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [78/191] | Loss: 0.3487 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [79/191] | Loss: 0.3559 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [80/191] | Loss: 0.3513 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [81/191] | Loss: 0.3619 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [82/191] | Loss: 0.3495 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [83/191] | Loss: 0.3492 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [84/191] | Loss: 0.3471 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [85/191] | Loss: 0.3485 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [86/191] | Loss: 0.3575 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [87/191] | Loss: 0.3491 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [88/191] | Loss: 0.3535 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [89/191] | Loss: 0.3496 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [90/191] | Loss: 0.3482 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [91/191] | Loss: 0.3496 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [92/191] | Loss: 0.3422 | Acc: 97.00%\n",
      "Epoch [8/10] | Batch [93/191] | Loss: 0.3619 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [94/191] | Loss: 0.3524 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [95/191] | Loss: 0.3592 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [96/191] | Loss: 0.3516 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [97/191] | Loss: 0.3563 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [98/191] | Loss: 0.3470 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [99/191] | Loss: 0.3611 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [100/191] | Loss: 0.3418 | Acc: 97.20%\n",
      "Epoch [8/10] | Batch [101/191] | Loss: 0.3704 | Acc: 93.80%\n",
      "Epoch [8/10] | Batch [102/191] | Loss: 0.3518 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [103/191] | Loss: 0.3412 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [104/191] | Loss: 0.3457 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [105/191] | Loss: 0.3647 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [106/191] | Loss: 0.3581 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [107/191] | Loss: 0.3602 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [108/191] | Loss: 0.3641 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [109/191] | Loss: 0.3626 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [110/191] | Loss: 0.3680 | Acc: 94.20%\n",
      "Epoch [8/10] | Batch [111/191] | Loss: 0.3488 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [112/191] | Loss: 0.3521 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [113/191] | Loss: 0.3479 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [114/191] | Loss: 0.3575 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [115/191] | Loss: 0.3574 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [116/191] | Loss: 0.3662 | Acc: 94.40%\n",
      "Epoch [8/10] | Batch [117/191] | Loss: 0.3672 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [118/191] | Loss: 0.3486 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [119/191] | Loss: 0.3662 | Acc: 94.60%\n",
      "Epoch [8/10] | Batch [120/191] | Loss: 0.3475 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [121/191] | Loss: 0.3609 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [122/191] | Loss: 0.3542 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [123/191] | Loss: 0.3433 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [124/191] | Loss: 0.3598 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [125/191] | Loss: 0.3604 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [126/191] | Loss: 0.3713 | Acc: 93.60%\n",
      "Epoch [8/10] | Batch [127/191] | Loss: 0.3653 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [128/191] | Loss: 0.3478 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [129/191] | Loss: 0.3583 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [130/191] | Loss: 0.3448 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [131/191] | Loss: 0.3519 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [132/191] | Loss: 0.3501 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [133/191] | Loss: 0.3428 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [134/191] | Loss: 0.3636 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [135/191] | Loss: 0.3608 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [136/191] | Loss: 0.3454 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [137/191] | Loss: 0.3578 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [138/191] | Loss: 0.3533 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [139/191] | Loss: 0.3571 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [140/191] | Loss: 0.3413 | Acc: 97.40%\n",
      "Epoch [8/10] | Batch [141/191] | Loss: 0.3511 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [142/191] | Loss: 0.3577 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [143/191] | Loss: 0.3582 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [144/191] | Loss: 0.3808 | Acc: 93.00%\n",
      "Epoch [8/10] | Batch [145/191] | Loss: 0.3478 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [146/191] | Loss: 0.3699 | Acc: 94.00%\n",
      "Epoch [8/10] | Batch [147/191] | Loss: 0.3329 | Acc: 97.80%\n",
      "Epoch [8/10] | Batch [148/191] | Loss: 0.3426 | Acc: 97.20%\n",
      "Epoch [8/10] | Batch [149/191] | Loss: 0.3536 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [150/191] | Loss: 0.3491 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [151/191] | Loss: 0.3604 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [152/191] | Loss: 0.3492 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [153/191] | Loss: 0.3537 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [154/191] | Loss: 0.3446 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [155/191] | Loss: 0.3502 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [156/191] | Loss: 0.3341 | Acc: 97.80%\n",
      "Epoch [8/10] | Batch [157/191] | Loss: 0.3479 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [158/191] | Loss: 0.3642 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [159/191] | Loss: 0.3624 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [160/191] | Loss: 0.3623 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [161/191] | Loss: 0.3421 | Acc: 97.20%\n",
      "Epoch [8/10] | Batch [162/191] | Loss: 0.3496 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [163/191] | Loss: 0.3547 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [164/191] | Loss: 0.3532 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [165/191] | Loss: 0.3496 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [166/191] | Loss: 0.3486 | Acc: 96.60%\n",
      "Epoch [8/10] | Batch [167/191] | Loss: 0.3465 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [168/191] | Loss: 0.3400 | Acc: 97.20%\n",
      "Epoch [8/10] | Batch [169/191] | Loss: 0.3590 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [170/191] | Loss: 0.3631 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [171/191] | Loss: 0.3494 | Acc: 96.40%\n",
      "Epoch [8/10] | Batch [172/191] | Loss: 0.3597 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [173/191] | Loss: 0.3557 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [174/191] | Loss: 0.3570 | Acc: 95.80%\n",
      "Epoch [8/10] | Batch [175/191] | Loss: 0.3449 | Acc: 96.80%\n",
      "Epoch [8/10] | Batch [176/191] | Loss: 0.3542 | Acc: 95.40%\n",
      "Epoch [8/10] | Batch [177/191] | Loss: 0.3429 | Acc: 97.00%\n",
      "Epoch [8/10] | Batch [178/191] | Loss: 0.3394 | Acc: 97.40%\n",
      "Epoch [8/10] | Batch [179/191] | Loss: 0.3518 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [180/191] | Loss: 0.3612 | Acc: 94.80%\n",
      "Epoch [8/10] | Batch [181/191] | Loss: 0.3497 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [182/191] | Loss: 0.3545 | Acc: 95.60%\n",
      "Epoch [8/10] | Batch [183/191] | Loss: 0.3598 | Acc: 95.20%\n",
      "Epoch [8/10] | Batch [184/191] | Loss: 0.3424 | Acc: 97.20%\n",
      "Epoch [8/10] | Batch [185/191] | Loss: 0.3548 | Acc: 96.00%\n",
      "Epoch [8/10] | Batch [186/191] | Loss: 0.3715 | Acc: 93.80%\n",
      "Epoch [8/10] | Batch [187/191] | Loss: 0.3519 | Acc: 96.20%\n",
      "Epoch [8/10] | Batch [188/191] | Loss: 0.3624 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [189/191] | Loss: 0.3619 | Acc: 95.00%\n",
      "Epoch [8/10] | Batch [190/191] | Loss: 0.3682 | Acc: 94.40%\n",
      "Epoch [8/10] | Batch [191/191] | Loss: 0.3518 | Acc: 96.20%\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Train Loss: 0.3517 | Acc: 95.36% | Precision: 0.9587 | Recall: 0.9585 | F1: 0.9585\n",
      "Test  Loss: 0.3573 | Acc: 95.45% | Precision: 0.9546 | Recall: 0.9546 | F1: 0.9545\n",
      "Learning Rate: 1.86e-06\n",
      "\n",
      "Epoch [9/10] | Batch [1/191] | Loss: 0.3522 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [2/191] | Loss: 0.3557 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [3/191] | Loss: 0.3642 | Acc: 94.60%\n",
      "Epoch [9/10] | Batch [4/191] | Loss: 0.3575 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [5/191] | Loss: 0.3436 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [6/191] | Loss: 0.3467 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [7/191] | Loss: 0.3626 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [8/191] | Loss: 0.3616 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [9/191] | Loss: 0.3558 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [10/191] | Loss: 0.3540 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [11/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [12/191] | Loss: 0.3580 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [13/191] | Loss: 0.3466 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [14/191] | Loss: 0.3523 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [15/191] | Loss: 0.3534 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [16/191] | Loss: 0.3568 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [17/191] | Loss: 0.3498 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [18/191] | Loss: 0.3555 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [19/191] | Loss: 0.3516 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [20/191] | Loss: 0.3589 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [21/191] | Loss: 0.3526 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [22/191] | Loss: 0.3461 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [23/191] | Loss: 0.3532 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [24/191] | Loss: 0.3495 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [25/191] | Loss: 0.3515 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [26/191] | Loss: 0.3506 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [27/191] | Loss: 0.3570 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [28/191] | Loss: 0.3426 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [29/191] | Loss: 0.3528 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [30/191] | Loss: 0.3687 | Acc: 94.60%\n",
      "Epoch [9/10] | Batch [31/191] | Loss: 0.3480 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [32/191] | Loss: 0.3399 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [33/191] | Loss: 0.3320 | Acc: 98.40%\n",
      "Epoch [9/10] | Batch [34/191] | Loss: 0.3525 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [35/191] | Loss: 0.3576 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [36/191] | Loss: 0.3414 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [37/191] | Loss: 0.3424 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [38/191] | Loss: 0.3450 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [39/191] | Loss: 0.3391 | Acc: 97.40%\n",
      "Epoch [9/10] | Batch [40/191] | Loss: 0.3457 | Acc: 97.00%\n",
      "Epoch [9/10] | Batch [41/191] | Loss: 0.3480 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [42/191] | Loss: 0.3553 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [43/191] | Loss: 0.3462 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [44/191] | Loss: 0.3497 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [45/191] | Loss: 0.3497 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [46/191] | Loss: 0.3557 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [47/191] | Loss: 0.3514 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [48/191] | Loss: 0.3574 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [49/191] | Loss: 0.3406 | Acc: 97.00%\n",
      "Epoch [9/10] | Batch [50/191] | Loss: 0.3574 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [51/191] | Loss: 0.3590 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [52/191] | Loss: 0.3492 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [53/191] | Loss: 0.3489 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [54/191] | Loss: 0.3501 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [55/191] | Loss: 0.3491 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [56/191] | Loss: 0.3520 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [57/191] | Loss: 0.3452 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [58/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [59/191] | Loss: 0.3595 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [60/191] | Loss: 0.3667 | Acc: 94.60%\n",
      "Epoch [9/10] | Batch [61/191] | Loss: 0.3652 | Acc: 94.80%\n",
      "Epoch [9/10] | Batch [62/191] | Loss: 0.3638 | Acc: 94.80%\n",
      "Epoch [9/10] | Batch [63/191] | Loss: 0.3407 | Acc: 97.40%\n",
      "Epoch [9/10] | Batch [64/191] | Loss: 0.3512 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [65/191] | Loss: 0.3482 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [66/191] | Loss: 0.3709 | Acc: 94.20%\n",
      "Epoch [9/10] | Batch [67/191] | Loss: 0.3602 | Acc: 95.00%\n",
      "Epoch [9/10] | Batch [68/191] | Loss: 0.3674 | Acc: 94.60%\n",
      "Epoch [9/10] | Batch [69/191] | Loss: 0.3490 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [70/191] | Loss: 0.3594 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [71/191] | Loss: 0.3425 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [72/191] | Loss: 0.3573 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [73/191] | Loss: 0.3402 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [74/191] | Loss: 0.3486 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [75/191] | Loss: 0.3465 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [76/191] | Loss: 0.3542 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [77/191] | Loss: 0.3486 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [78/191] | Loss: 0.3600 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [79/191] | Loss: 0.3729 | Acc: 94.00%\n",
      "Epoch [9/10] | Batch [80/191] | Loss: 0.3629 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [81/191] | Loss: 0.3537 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [82/191] | Loss: 0.3620 | Acc: 94.80%\n",
      "Epoch [9/10] | Batch [83/191] | Loss: 0.3559 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [84/191] | Loss: 0.3418 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [85/191] | Loss: 0.3561 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [86/191] | Loss: 0.3469 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [87/191] | Loss: 0.3586 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [88/191] | Loss: 0.3515 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [89/191] | Loss: 0.3510 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [90/191] | Loss: 0.3530 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [91/191] | Loss: 0.3538 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [92/191] | Loss: 0.3546 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [93/191] | Loss: 0.3506 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [94/191] | Loss: 0.3584 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [95/191] | Loss: 0.3528 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [96/191] | Loss: 0.3420 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [97/191] | Loss: 0.3535 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [98/191] | Loss: 0.3417 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [99/191] | Loss: 0.3481 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [100/191] | Loss: 0.3503 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [101/191] | Loss: 0.3586 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [102/191] | Loss: 0.3504 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [103/191] | Loss: 0.3471 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [104/191] | Loss: 0.3519 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [105/191] | Loss: 0.3430 | Acc: 97.00%\n",
      "Epoch [9/10] | Batch [106/191] | Loss: 0.3643 | Acc: 95.00%\n",
      "Epoch [9/10] | Batch [107/191] | Loss: 0.3493 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [108/191] | Loss: 0.3494 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [109/191] | Loss: 0.3415 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [110/191] | Loss: 0.3495 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [111/191] | Loss: 0.3522 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [112/191] | Loss: 0.3551 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [113/191] | Loss: 0.3425 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [114/191] | Loss: 0.3603 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [115/191] | Loss: 0.3465 | Acc: 97.00%\n",
      "Epoch [9/10] | Batch [116/191] | Loss: 0.3550 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [117/191] | Loss: 0.3584 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [118/191] | Loss: 0.3542 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [119/191] | Loss: 0.3604 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [120/191] | Loss: 0.3440 | Acc: 97.00%\n",
      "Epoch [9/10] | Batch [121/191] | Loss: 0.3667 | Acc: 94.40%\n",
      "Epoch [9/10] | Batch [122/191] | Loss: 0.3587 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [123/191] | Loss: 0.3564 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [124/191] | Loss: 0.3614 | Acc: 95.00%\n",
      "Epoch [9/10] | Batch [125/191] | Loss: 0.3463 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [126/191] | Loss: 0.3583 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [127/191] | Loss: 0.3355 | Acc: 97.80%\n",
      "Epoch [9/10] | Batch [128/191] | Loss: 0.3413 | Acc: 97.00%\n",
      "Epoch [9/10] | Batch [129/191] | Loss: 0.3633 | Acc: 94.80%\n",
      "Epoch [9/10] | Batch [130/191] | Loss: 0.3523 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [131/191] | Loss: 0.3543 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [132/191] | Loss: 0.3556 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [133/191] | Loss: 0.3592 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [134/191] | Loss: 0.3490 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [135/191] | Loss: 0.3696 | Acc: 94.40%\n",
      "Epoch [9/10] | Batch [136/191] | Loss: 0.3532 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [137/191] | Loss: 0.3513 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [138/191] | Loss: 0.3457 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [139/191] | Loss: 0.3531 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [140/191] | Loss: 0.3713 | Acc: 94.20%\n",
      "Epoch [9/10] | Batch [141/191] | Loss: 0.3527 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [142/191] | Loss: 0.3585 | Acc: 95.00%\n",
      "Epoch [9/10] | Batch [143/191] | Loss: 0.3357 | Acc: 97.60%\n",
      "Epoch [9/10] | Batch [144/191] | Loss: 0.3545 | Acc: 95.80%\n",
      "Epoch [9/10] | Batch [145/191] | Loss: 0.3540 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [146/191] | Loss: 0.3469 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [147/191] | Loss: 0.3463 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [148/191] | Loss: 0.3535 | Acc: 95.60%\n",
      "Epoch [9/10] | Batch [149/191] | Loss: 0.3499 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [150/191] | Loss: 0.3531 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [151/191] | Loss: 0.3414 | Acc: 97.00%\n",
      "Epoch [9/10] | Batch [152/191] | Loss: 0.3583 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [153/191] | Loss: 0.3658 | Acc: 94.60%\n",
      "Epoch [9/10] | Batch [154/191] | Loss: 0.3571 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [155/191] | Loss: 0.3599 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [156/191] | Loss: 0.3480 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [157/191] | Loss: 0.3477 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [158/191] | Loss: 0.3605 | Acc: 94.80%\n",
      "Epoch [9/10] | Batch [159/191] | Loss: 0.3568 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [160/191] | Loss: 0.3420 | Acc: 97.20%\n",
      "Epoch [9/10] | Batch [161/191] | Loss: 0.3526 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [162/191] | Loss: 0.3487 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [163/191] | Loss: 0.3512 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [164/191] | Loss: 0.3326 | Acc: 97.80%\n",
      "Epoch [9/10] | Batch [165/191] | Loss: 0.3474 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [166/191] | Loss: 0.3616 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [167/191] | Loss: 0.3504 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [168/191] | Loss: 0.3562 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [169/191] | Loss: 0.3497 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [170/191] | Loss: 0.3576 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [171/191] | Loss: 0.3585 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [172/191] | Loss: 0.3607 | Acc: 95.00%\n",
      "Epoch [9/10] | Batch [173/191] | Loss: 0.3602 | Acc: 95.00%\n",
      "Epoch [9/10] | Batch [174/191] | Loss: 0.3500 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [175/191] | Loss: 0.3632 | Acc: 94.80%\n",
      "Epoch [9/10] | Batch [176/191] | Loss: 0.3670 | Acc: 94.60%\n",
      "Epoch [9/10] | Batch [177/191] | Loss: 0.3469 | Acc: 96.80%\n",
      "Epoch [9/10] | Batch [178/191] | Loss: 0.3486 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [179/191] | Loss: 0.3498 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [180/191] | Loss: 0.3467 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [181/191] | Loss: 0.3343 | Acc: 97.80%\n",
      "Epoch [9/10] | Batch [182/191] | Loss: 0.3642 | Acc: 94.60%\n",
      "Epoch [9/10] | Batch [183/191] | Loss: 0.3575 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [184/191] | Loss: 0.3605 | Acc: 95.20%\n",
      "Epoch [9/10] | Batch [185/191] | Loss: 0.3506 | Acc: 96.40%\n",
      "Epoch [9/10] | Batch [186/191] | Loss: 0.3524 | Acc: 96.20%\n",
      "Epoch [9/10] | Batch [187/191] | Loss: 0.3466 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [188/191] | Loss: 0.3536 | Acc: 96.00%\n",
      "Epoch [9/10] | Batch [189/191] | Loss: 0.3479 | Acc: 96.60%\n",
      "Epoch [9/10] | Batch [190/191] | Loss: 0.3595 | Acc: 95.40%\n",
      "Epoch [9/10] | Batch [191/191] | Loss: 0.3527 | Acc: 95.80%\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Train Loss: 0.3509 | Acc: 95.47% | Precision: 0.9599 | Recall: 0.9596 | F1: 0.9596\n",
      "Test  Loss: 0.3570 | Acc: 95.45% | Precision: 0.9547 | Recall: 0.9546 | F1: 0.9545\n",
      "Learning Rate: 1.22e-06\n",
      "\n",
      "Epoch [10/10] | Batch [1/191] | Loss: 0.3717 | Acc: 93.80%\n",
      "Epoch [10/10] | Batch [2/191] | Loss: 0.3446 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [3/191] | Loss: 0.3527 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [4/191] | Loss: 0.3559 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [5/191] | Loss: 0.3503 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [6/191] | Loss: 0.3515 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [7/191] | Loss: 0.3512 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [8/191] | Loss: 0.3577 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [9/191] | Loss: 0.3552 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [10/191] | Loss: 0.3413 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [11/191] | Loss: 0.3540 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [12/191] | Loss: 0.3514 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [13/191] | Loss: 0.3504 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [14/191] | Loss: 0.3364 | Acc: 98.20%\n",
      "Epoch [10/10] | Batch [15/191] | Loss: 0.3456 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [16/191] | Loss: 0.3553 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [17/191] | Loss: 0.3447 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [18/191] | Loss: 0.3643 | Acc: 95.00%\n",
      "Epoch [10/10] | Batch [19/191] | Loss: 0.3574 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [20/191] | Loss: 0.3321 | Acc: 98.20%\n",
      "Epoch [10/10] | Batch [21/191] | Loss: 0.3607 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [22/191] | Loss: 0.3538 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [23/191] | Loss: 0.3572 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [24/191] | Loss: 0.3646 | Acc: 94.80%\n",
      "Epoch [10/10] | Batch [25/191] | Loss: 0.3454 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [26/191] | Loss: 0.3453 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [27/191] | Loss: 0.3641 | Acc: 94.40%\n",
      "Epoch [10/10] | Batch [28/191] | Loss: 0.3623 | Acc: 94.20%\n",
      "Epoch [10/10] | Batch [29/191] | Loss: 0.3427 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [30/191] | Loss: 0.3449 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [31/191] | Loss: 0.3525 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [32/191] | Loss: 0.3622 | Acc: 95.00%\n",
      "Epoch [10/10] | Batch [33/191] | Loss: 0.3433 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [34/191] | Loss: 0.3510 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [35/191] | Loss: 0.3577 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [36/191] | Loss: 0.3506 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [37/191] | Loss: 0.3586 | Acc: 95.00%\n",
      "Epoch [10/10] | Batch [38/191] | Loss: 0.3550 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [39/191] | Loss: 0.3555 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [40/191] | Loss: 0.3418 | Acc: 97.40%\n",
      "Epoch [10/10] | Batch [41/191] | Loss: 0.3395 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [42/191] | Loss: 0.3507 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [43/191] | Loss: 0.3549 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [44/191] | Loss: 0.3467 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [45/191] | Loss: 0.3488 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [46/191] | Loss: 0.3418 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [47/191] | Loss: 0.3607 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [48/191] | Loss: 0.3512 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [49/191] | Loss: 0.3587 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [50/191] | Loss: 0.3416 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [51/191] | Loss: 0.3410 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [52/191] | Loss: 0.3527 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [53/191] | Loss: 0.3551 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [54/191] | Loss: 0.3615 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [55/191] | Loss: 0.3650 | Acc: 94.60%\n",
      "Epoch [10/10] | Batch [56/191] | Loss: 0.3491 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [57/191] | Loss: 0.3443 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [58/191] | Loss: 0.3418 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [59/191] | Loss: 0.3535 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [60/191] | Loss: 0.3529 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [61/191] | Loss: 0.3373 | Acc: 97.80%\n",
      "Epoch [10/10] | Batch [62/191] | Loss: 0.3592 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [63/191] | Loss: 0.3422 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [64/191] | Loss: 0.3561 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [65/191] | Loss: 0.3481 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [66/191] | Loss: 0.3677 | Acc: 94.80%\n",
      "Epoch [10/10] | Batch [67/191] | Loss: 0.3590 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [68/191] | Loss: 0.3545 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [69/191] | Loss: 0.3583 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [70/191] | Loss: 0.3555 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [71/191] | Loss: 0.3508 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [72/191] | Loss: 0.3554 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [73/191] | Loss: 0.3453 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [74/191] | Loss: 0.3448 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [75/191] | Loss: 0.3729 | Acc: 93.80%\n",
      "Epoch [10/10] | Batch [76/191] | Loss: 0.3611 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [77/191] | Loss: 0.3544 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [78/191] | Loss: 0.3485 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [79/191] | Loss: 0.3541 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [80/191] | Loss: 0.3446 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [81/191] | Loss: 0.3454 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [82/191] | Loss: 0.3597 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [83/191] | Loss: 0.3592 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [84/191] | Loss: 0.3462 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [85/191] | Loss: 0.3475 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [86/191] | Loss: 0.3428 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [87/191] | Loss: 0.3619 | Acc: 95.00%\n",
      "Epoch [10/10] | Batch [88/191] | Loss: 0.3513 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [89/191] | Loss: 0.3521 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [90/191] | Loss: 0.3470 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [91/191] | Loss: 0.3461 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [92/191] | Loss: 0.3728 | Acc: 94.00%\n",
      "Epoch [10/10] | Batch [93/191] | Loss: 0.3399 | Acc: 97.40%\n",
      "Epoch [10/10] | Batch [94/191] | Loss: 0.3439 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [95/191] | Loss: 0.3459 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [96/191] | Loss: 0.3753 | Acc: 93.80%\n",
      "Epoch [10/10] | Batch [97/191] | Loss: 0.3483 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [98/191] | Loss: 0.3556 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [99/191] | Loss: 0.3486 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [100/191] | Loss: 0.3418 | Acc: 97.40%\n",
      "Epoch [10/10] | Batch [101/191] | Loss: 0.3551 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [102/191] | Loss: 0.3394 | Acc: 97.40%\n",
      "Epoch [10/10] | Batch [103/191] | Loss: 0.3653 | Acc: 94.80%\n",
      "Epoch [10/10] | Batch [104/191] | Loss: 0.3412 | Acc: 97.40%\n",
      "Epoch [10/10] | Batch [105/191] | Loss: 0.3626 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [106/191] | Loss: 0.3445 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [107/191] | Loss: 0.3422 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [108/191] | Loss: 0.3487 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [109/191] | Loss: 0.3488 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [110/191] | Loss: 0.3420 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [111/191] | Loss: 0.3433 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [112/191] | Loss: 0.3464 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [113/191] | Loss: 0.3570 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [114/191] | Loss: 0.3493 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [115/191] | Loss: 0.3487 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [116/191] | Loss: 0.3526 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [117/191] | Loss: 0.3543 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [118/191] | Loss: 0.3376 | Acc: 97.60%\n",
      "Epoch [10/10] | Batch [119/191] | Loss: 0.3637 | Acc: 94.80%\n",
      "Epoch [10/10] | Batch [120/191] | Loss: 0.3486 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [121/191] | Loss: 0.3422 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [122/191] | Loss: 0.3543 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [123/191] | Loss: 0.3547 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [124/191] | Loss: 0.3337 | Acc: 98.00%\n",
      "Epoch [10/10] | Batch [125/191] | Loss: 0.3624 | Acc: 95.00%\n",
      "Epoch [10/10] | Batch [126/191] | Loss: 0.3653 | Acc: 94.60%\n",
      "Epoch [10/10] | Batch [127/191] | Loss: 0.3598 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [128/191] | Loss: 0.3664 | Acc: 94.40%\n",
      "Epoch [10/10] | Batch [129/191] | Loss: 0.3776 | Acc: 93.40%\n",
      "Epoch [10/10] | Batch [130/191] | Loss: 0.3510 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [131/191] | Loss: 0.3570 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [132/191] | Loss: 0.3441 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [133/191] | Loss: 0.3539 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [134/191] | Loss: 0.3464 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [135/191] | Loss: 0.3427 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [136/191] | Loss: 0.3553 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [137/191] | Loss: 0.3693 | Acc: 94.00%\n",
      "Epoch [10/10] | Batch [138/191] | Loss: 0.3566 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [139/191] | Loss: 0.3472 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [140/191] | Loss: 0.3536 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [141/191] | Loss: 0.3531 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [142/191] | Loss: 0.3633 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [143/191] | Loss: 0.3417 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [144/191] | Loss: 0.3726 | Acc: 94.00%\n",
      "Epoch [10/10] | Batch [145/191] | Loss: 0.3546 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [146/191] | Loss: 0.3481 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [147/191] | Loss: 0.3618 | Acc: 95.00%\n",
      "Epoch [10/10] | Batch [148/191] | Loss: 0.3611 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [149/191] | Loss: 0.3519 | Acc: 96.20%\n",
      "Epoch [10/10] | Batch [150/191] | Loss: 0.3484 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [151/191] | Loss: 0.3506 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [152/191] | Loss: 0.3351 | Acc: 97.80%\n",
      "Epoch [10/10] | Batch [153/191] | Loss: 0.3452 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [154/191] | Loss: 0.3562 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [155/191] | Loss: 0.3640 | Acc: 94.80%\n",
      "Epoch [10/10] | Batch [156/191] | Loss: 0.3519 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [157/191] | Loss: 0.3558 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [158/191] | Loss: 0.3494 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [159/191] | Loss: 0.3356 | Acc: 97.60%\n",
      "Epoch [10/10] | Batch [160/191] | Loss: 0.3460 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [161/191] | Loss: 0.3581 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [162/191] | Loss: 0.3587 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [163/191] | Loss: 0.3461 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [164/191] | Loss: 0.3482 | Acc: 96.40%\n",
      "Epoch [10/10] | Batch [165/191] | Loss: 0.3698 | Acc: 94.20%\n",
      "Epoch [10/10] | Batch [166/191] | Loss: 0.3521 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [167/191] | Loss: 0.3596 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [168/191] | Loss: 0.3597 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [169/191] | Loss: 0.3464 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [170/191] | Loss: 0.3538 | Acc: 95.60%\n",
      "Epoch [10/10] | Batch [171/191] | Loss: 0.3521 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [172/191] | Loss: 0.3462 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [173/191] | Loss: 0.3454 | Acc: 97.00%\n",
      "Epoch [10/10] | Batch [174/191] | Loss: 0.3531 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [175/191] | Loss: 0.3598 | Acc: 95.40%\n",
      "Epoch [10/10] | Batch [176/191] | Loss: 0.3595 | Acc: 95.00%\n",
      "Epoch [10/10] | Batch [177/191] | Loss: 0.3545 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [178/191] | Loss: 0.3429 | Acc: 96.80%\n",
      "Epoch [10/10] | Batch [179/191] | Loss: 0.3412 | Acc: 97.60%\n",
      "Epoch [10/10] | Batch [180/191] | Loss: 0.3475 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [181/191] | Loss: 0.3438 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [182/191] | Loss: 0.3410 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [183/191] | Loss: 0.3590 | Acc: 95.20%\n",
      "Epoch [10/10] | Batch [184/191] | Loss: 0.3398 | Acc: 97.40%\n",
      "Epoch [10/10] | Batch [185/191] | Loss: 0.3478 | Acc: 96.60%\n",
      "Epoch [10/10] | Batch [186/191] | Loss: 0.3522 | Acc: 95.80%\n",
      "Epoch [10/10] | Batch [187/191] | Loss: 0.3423 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [188/191] | Loss: 0.3425 | Acc: 97.20%\n",
      "Epoch [10/10] | Batch [189/191] | Loss: 0.3648 | Acc: 94.60%\n",
      "Epoch [10/10] | Batch [190/191] | Loss: 0.3541 | Acc: 96.00%\n",
      "Epoch [10/10] | Batch [191/191] | Loss: 0.3269 | Acc: 98.60%\n",
      "\n",
      "Epoch 10 Summary:\n",
      "Train Loss: 0.3501 | Acc: 95.58% | Precision: 0.9609 | Recall: 0.9607 | F1: 0.9607\n",
      "Test  Loss: 0.3568 | Acc: 95.54% | Precision: 0.9555 | Recall: 0.9555 | F1: 0.9554\n",
      "Learning Rate: 1.00e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f9adb5-cb4e-4032-8da5-136101a7506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9553, 0.0447]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(text):\n",
    "    # 初始化模型\n",
    "    model = Bert(args.vocab_size, args.d_model, args.num_heads, \n",
    "                args.num_layers, args.d_ff, args.max_len, args.dropout)\n",
    "    \n",
    "    # 加载预训练权重（关键新增部分）\n",
    "    model.load_state_dict(torch.load('model_epoch_2.pth'))  # 替换为你的权重路径\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 文本编码\n",
    "        text = tokenizer.encode(text, \n",
    "                              add_special_tokens=True,\n",
    "                              max_length=100,\n",
    "                              padding='max_length',\n",
    "                              truncation=True)\n",
    "        text = torch.tensor(text, dtype=torch.long).unsqueeze(0)\n",
    "        \n",
    "        # 前向传播\n",
    "        output = model(text)\n",
    "        print(output)\n",
    "        # 获取预测结果（修正了参数缺失问题）\n",
    "        preds = torch.argmax(output, dim=-1)  # 添加了output参数\n",
    "        \n",
    "        return preds\n",
    "predict('。。。。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122eaa5a-f15a-47ee-86c6-74833ac83005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c24f1-189c-4bc5-af13-4bde768c9e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a01911-59b6-44a0-81dd-8ad016409840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
